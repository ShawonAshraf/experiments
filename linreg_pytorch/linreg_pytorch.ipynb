{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.12 64-bit ('experiments')",
   "display_name": "Python 3.6.12 64-bit ('experiments')",
   "metadata": {
    "interpreter": {
     "hash": "0c259131826c942fe3ad44e5381c0ded0b77f58c735f947dd41e948c68227dad"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>MEDV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "dataset = load_boston()\n",
    "dataset = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "dataset['MEDV'] = load_boston().target\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this is an experiment, picking features from this post\n",
    "# https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = pd.DataFrame(np.c_[dataset['LSTAT'], dataset['RM']], columns = ['LSTAT','RM']).values\n",
    "Y = dataset['MEDV'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "linm = nn.Linear(input_size, output_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linm.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/150], Loss: 590.3761\nEpoch [2/150], Loss: 377.9886\nEpoch [3/150], Loss: 314.2489\nEpoch [4/150], Loss: 289.2007\nEpoch [5/150], Loss: 274.5026\nEpoch [6/150], Loss: 262.8373\nEpoch [7/150], Loss: 252.3034\nEpoch [8/150], Loss: 242.3958\nEpoch [9/150], Loss: 232.9692\nEpoch [10/150], Loss: 223.9722\nEpoch [11/150], Loss: 215.3779\nEpoch [12/150], Loss: 207.1664\nEpoch [13/150], Loss: 199.3200\nEpoch [14/150], Loss: 191.8226\nEpoch [15/150], Loss: 184.6584\nEpoch [16/150], Loss: 177.8128\nEpoch [17/150], Loss: 171.2716\nEpoch [18/150], Loss: 165.0211\nEpoch [19/150], Loss: 159.0485\nEpoch [20/150], Loss: 153.3414\nEpoch [21/150], Loss: 147.8881\nEpoch [22/150], Loss: 142.6772\nEpoch [23/150], Loss: 137.6980\nEpoch [24/150], Loss: 132.9401\nEpoch [25/150], Loss: 128.3937\nEpoch [26/150], Loss: 124.0495\nEpoch [27/150], Loss: 119.8984\nEpoch [28/150], Loss: 115.9318\nEpoch [29/150], Loss: 112.1416\nEpoch [30/150], Loss: 108.5199\nEpoch [31/150], Loss: 105.0591\nEpoch [32/150], Loss: 101.7523\nEpoch [33/150], Loss: 98.5924\nEpoch [34/150], Loss: 95.5731\nEpoch [35/150], Loss: 92.6879\nEpoch [36/150], Loss: 89.9310\nEpoch [37/150], Loss: 87.2968\nEpoch [38/150], Loss: 84.7796\nEpoch [39/150], Loss: 82.3743\nEpoch [40/150], Loss: 80.0759\nEpoch [41/150], Loss: 77.8798\nEpoch [42/150], Loss: 75.7812\nEpoch [43/150], Loss: 73.7760\nEpoch [44/150], Loss: 71.8599\nEpoch [45/150], Loss: 70.0290\nEpoch [46/150], Loss: 68.2795\nEpoch [47/150], Loss: 66.6077\nEpoch [48/150], Loss: 65.0103\nEpoch [49/150], Loss: 63.4839\nEpoch [50/150], Loss: 62.0254\nEpoch [51/150], Loss: 60.6317\nEpoch [52/150], Loss: 59.2999\nEpoch [53/150], Loss: 58.0274\nEpoch [54/150], Loss: 56.8114\nEpoch [55/150], Loss: 55.6495\nEpoch [56/150], Loss: 54.5393\nEpoch [57/150], Loss: 53.4784\nEpoch [58/150], Loss: 52.4647\nEpoch [59/150], Loss: 51.4960\nEpoch [60/150], Loss: 50.5704\nEpoch [61/150], Loss: 49.6860\nEpoch [62/150], Loss: 48.8408\nEpoch [63/150], Loss: 48.0333\nEpoch [64/150], Loss: 47.2616\nEpoch [65/150], Loss: 46.5243\nEpoch [66/150], Loss: 45.8197\nEpoch [67/150], Loss: 45.1465\nEpoch [68/150], Loss: 44.5032\nEpoch [69/150], Loss: 43.8884\nEpoch [70/150], Loss: 43.3011\nEpoch [71/150], Loss: 42.7398\nEpoch [72/150], Loss: 42.2035\nEpoch [73/150], Loss: 41.6910\nEpoch [74/150], Loss: 41.2013\nEpoch [75/150], Loss: 40.7334\nEpoch [76/150], Loss: 40.2863\nEpoch [77/150], Loss: 39.8590\nEpoch [78/150], Loss: 39.4508\nEpoch [79/150], Loss: 39.0607\nEpoch [80/150], Loss: 38.6879\nEpoch [81/150], Loss: 38.3317\nEpoch [82/150], Loss: 37.9914\nEpoch [83/150], Loss: 37.6661\nEpoch [84/150], Loss: 37.3554\nEpoch [85/150], Loss: 37.0585\nEpoch [86/150], Loss: 36.7747\nEpoch [87/150], Loss: 36.5036\nEpoch [88/150], Loss: 36.2445\nEpoch [89/150], Loss: 35.9969\nEpoch [90/150], Loss: 35.7604\nEpoch [91/150], Loss: 35.5343\nEpoch [92/150], Loss: 35.3184\nEpoch [93/150], Loss: 35.1120\nEpoch [94/150], Loss: 34.9147\nEpoch [95/150], Loss: 34.7263\nEpoch [96/150], Loss: 34.5462\nEpoch [97/150], Loss: 34.3742\nEpoch [98/150], Loss: 34.2098\nEpoch [99/150], Loss: 34.0527\nEpoch [100/150], Loss: 33.9025\nEpoch [101/150], Loss: 33.7591\nEpoch [102/150], Loss: 33.6220\nEpoch [103/150], Loss: 33.4910\nEpoch [104/150], Loss: 33.3659\nEpoch [105/150], Loss: 33.2463\nEpoch [106/150], Loss: 33.1320\nEpoch [107/150], Loss: 33.0229\nEpoch [108/150], Loss: 32.9185\nEpoch [109/150], Loss: 32.8188\nEpoch [110/150], Loss: 32.7235\nEpoch [111/150], Loss: 32.6325\nEpoch [112/150], Loss: 32.5455\nEpoch [113/150], Loss: 32.4624\nEpoch [114/150], Loss: 32.3830\nEpoch [115/150], Loss: 32.3071\nEpoch [116/150], Loss: 32.2346\nEpoch [117/150], Loss: 32.1653\nEpoch [118/150], Loss: 32.0991\nEpoch [119/150], Loss: 32.0358\nEpoch [120/150], Loss: 31.9753\nEpoch [121/150], Loss: 31.9176\nEpoch [122/150], Loss: 31.8624\nEpoch [123/150], Loss: 31.8096\nEpoch [124/150], Loss: 31.7592\nEpoch [125/150], Loss: 31.7111\nEpoch [126/150], Loss: 31.6650\nEpoch [127/150], Loss: 31.6211\nEpoch [128/150], Loss: 31.5790\nEpoch [129/150], Loss: 31.5389\nEpoch [130/150], Loss: 31.5005\nEpoch [131/150], Loss: 31.4638\nEpoch [132/150], Loss: 31.4288\nEpoch [133/150], Loss: 31.3954\nEpoch [134/150], Loss: 31.3634\nEpoch [135/150], Loss: 31.3328\nEpoch [136/150], Loss: 31.3036\nEpoch [137/150], Loss: 31.2757\nEpoch [138/150], Loss: 31.2490\nEpoch [139/150], Loss: 31.2235\nEpoch [140/150], Loss: 31.1992\nEpoch [141/150], Loss: 31.1759\nEpoch [142/150], Loss: 31.1537\nEpoch [143/150], Loss: 31.1324\nEpoch [144/150], Loss: 31.1121\nEpoch [145/150], Loss: 31.0927\nEpoch [146/150], Loss: 31.0742\nEpoch [147/150], Loss: 31.0565\nEpoch [148/150], Loss: 31.0396\nEpoch [149/150], Loss: 31.0234\nEpoch [150/150], Loss: 31.0079\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "\n",
    "for e in range(epochs):\n",
    "    inputs = torch.from_numpy(X_train).float()\n",
    "    targets = torch.from_numpy(y_train).float()\n",
    "\n",
    "\n",
    "    # forwards pass\n",
    "    outputs = linm(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # backwards, GD\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(e + 1, epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 on train set :  0.6428088706865092\nR2 on test set :  0.603445833033869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# train set\n",
    "predicted = linm(torch.from_numpy(X_train).float()).detach().numpy()\n",
    "acc = r2_score(y_pred=predicted, y_true=y_train)\n",
    "print(\"R2 on train set : \", acc)\n",
    "\n",
    "\n",
    "# test set\n",
    "predicted = linm(torch.from_numpy(X_test).float()).detach().numpy()\n",
    "acc = r2_score(y_pred=predicted, y_true=y_test)\n",
    "print(\"R2 on test set : \", acc)\n"
   ]
  }
 ]
}