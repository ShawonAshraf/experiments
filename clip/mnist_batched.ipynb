{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawon/miniconda3/envs/exp/lib/python3.10/site-packages/clip/clip.py:57: UserWarning: /home/shawon/.cache/clip/RN101.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
      "100%|███████████████████████████████████████| 278M/278M [01:19<00:00, 3.68MiB/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('RN101', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "dataset = MNIST(root=\"./dataset\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - zero',\n",
       " '1 - one',\n",
       " '2 - two',\n",
       " '3 - three',\n",
       " '4 - four',\n",
       " '5 - five',\n",
       " '6 - six',\n",
       " '7 - seven',\n",
       " '8 - eight',\n",
       " '9 - nine']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,     1,     2,  ..., 59997, 59998, 59999])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_indices = torch.arange(len(dataset))\n",
    "all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_acc(predicted, actual):\n",
    "    total = (predicted == actual).count_nonzero().sum()\n",
    "    return total / predicted.size(0)\n",
    "\n",
    "batch_acc(torch.arange(10), torch.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def infer(text, batch_size=512, all_indices=all_indices) -> None:\n",
    "    batched_indices = torch.split(all_indices, batch_size)\n",
    "    n_batches = len(batched_indices)\n",
    "    batch_wise_acc = torch.zeros(n_batches, )\n",
    "    \n",
    "    for batch_idx, indices in tqdm(enumerate(batched_indices), total=n_batches):\n",
    "        images = [dataset[int(i)][0] for i in indices]\n",
    "        labels = [dataset[int(i)][1] for i in indices]\n",
    "        labels = torch.tensor(labels).float()\n",
    "    \n",
    "        images = torch.stack([\n",
    "            preprocess(image) for image in images\n",
    "        ], dim=0)\n",
    "    \n",
    "    \n",
    "        images = images.to(device)\n",
    "        text = text.to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            logits_per_image, _ = model(images, text)\n",
    "        \n",
    "        probs = logits_per_image.softmax(dim=-1).cpu()\n",
    "        pred = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    \n",
    "        batch_wise_acc[batch_idx] = batch_acc(pred, labels)\n",
    "        \n",
    "    # over all the batches\n",
    "    mean_acc =  batch_wise_acc.mean(dim=-1)\n",
    "    print(f\"Mean Accuracy :: {mean_acc.item()} over :: {n_batches} batches.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f7ac81703c4eb4b1f7642daa841f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy :: 0.5203092098236084 over :: 118 batches.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/openai/CLIP/issues/164\n",
    "label_prompts = [\n",
    "    f\"a photo of the number: '{i}'.\"\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "text = clip.tokenize(label_prompts)\n",
    "infer(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79335be75ee6449191b02471c6e06734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy :: 0.47272247076034546 over :: 118 batches.\n"
     ]
    }
   ],
   "source": [
    "label_prompts = [\n",
    "    f\"an image of the handwritten form of the number: '{i}'.\"\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "text = clip.tokenize(label_prompts)\n",
    "infer(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
