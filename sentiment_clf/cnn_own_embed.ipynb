{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "Polarity Dataset. Pang/Lee ACL 2004\n",
    "\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text file Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_path = \"./review_polarity/txt_sentoken/pos\"\n",
    "neg_path = \"./review_polarity/txt_sentoken/neg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_text_files(category_path):\n",
    "    file_list = os.listdir(category_path)\n",
    "    texts = []\n",
    "\n",
    "    for fname in file_list:\n",
    "        with open(os.path.join(category_path, fname), \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            texts.extend(lines)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "- tokenize\n",
    "- Clean punctuation, stopwords\n",
    "- Assign labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "ignore = stopwords.words(\"english\")\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    x = [token for token in tokens if token not in ignore]\n",
    "    return x\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    x = [token for token in tokens if token not in punctuation]\n",
    "    return x\n",
    "\n",
    "def clean(tokens):\n",
    "    x = remove_stopwords(text)\n",
    "    x = remove_punctuation(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# label -> 1 for pos and 0 for neg\n",
    "def prepare_corpus():\n",
    "    # dataset is a list of tuples\n",
    "    # (label, tokens)\n",
    "    corpus = list()\n",
    "\n",
    "    # idx -> label\n",
    "    text_paths = [neg_path, pos_path]\n",
    "    for idx, path in enumerate(text_paths):\n",
    "        texts = read_text_files(path)\n",
    "        for i in tqdm(range(len(texts)), desc=\"prepare_corpus\"):\n",
    "            text = texts[i]\n",
    "            # tokenize\n",
    "            tokens = tokenize(text)\n",
    "            \n",
    "            # clean\n",
    "            tokens = remove_punctuation(tokens)\n",
    "            tokens = remove_stopwords(tokens)\n",
    "            \n",
    "            # append\n",
    "            corpus.append((idx, tokens))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "prepare_corpus: 100%|██████████| 31783/31783 [00:04<00:00, 6839.70it/s]\n",
      "prepare_corpus: 100%|██████████| 32937/32937 [00:05<00:00, 6329.68it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = prepare_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0,\n",
       " ['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive'])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding - Word2Vec\n",
    "\n",
    "Can either try pretraiend embeddings from google news dataset or train a new embedding from the existing corpus. Let's try it on the existing corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wall time: 13.6 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(32380340, 37305350)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [s[1] for s in corpus]\n",
    "\n",
    "# 150 dims for the embedding\n",
    "# embedding dim will define the number of outputs per convolution!\n",
    "EMBEDDING_DIMS = 150\n",
    "\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# keep one cpu core free or some operating systems may kill the process :P\n",
    "# ignore words which have a frequency < 5\n",
    "corpus_embedding = Word2Vec(\n",
    "    vector_size=EMBEDDING_DIMS, \n",
    "    workers=cores-1, max_vocab_size=100000,\n",
    "    min_count=5)\n",
    "\n",
    "# build vocab\n",
    "corpus_embedding.build_vocab(sentences)\n",
    "\n",
    "# train for 50 epochs, can always change later!\n",
    "%time corpus_embedding.train(sentences, total_examples=len(sentences), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all the words word2vec has processed\n",
    "vocabulary = corpus_embedding.wv.index_to_key\n",
    "vocab_len = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14718"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"'s\", '``', 'film', \"n't\", 'movie', 'one', 'like', 'even', 'good', 'time']"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "source": [
    "## Encode all tokens with indices from embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_corpus_tokens_with_embed_idx(corpus):\n",
    "    encoded_corpus = list()\n",
    "    for i in tqdm(range(len(corpus)), desc=\"encode_tokens_with_embed_idx\"):\n",
    "        idxs = []\n",
    "        label, tokens = corpus[i]\n",
    "\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                idx = corpus_embedding.wv.key_to_index[token]\n",
    "            except:\n",
    "                # if token isn't in the vocab\n",
    "                idx = 0\n",
    "\n",
    "            idxs.append(idx)\n",
    "        \n",
    "        \n",
    "        encoded_corpus.append((label, idxs))\n",
    "\n",
    "\n",
    "\n",
    "    return encoded_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "encode_tokens_with_embed_idx: 100%|██████████| 64720/64720 [00:00<00:00, 317691.92it/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_corpus = encode_corpus_tokens_with_embed_idx(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[29, 18, 944, 4129, 45, 1854, 658, 3626, 1280]"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "encoded_corpus[0][1]"
   ]
  },
  {
   "source": [
    "## Padding\n",
    "\n",
    "Left pad with 0\n",
    "\n",
    "However we need a sequence length. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "# get max sequences length\n",
    "max_seq_len = max(len(s) for s in sentences)\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_tokens(encoded_corpus, seq_len=max_seq_len):\n",
    "    padded = np.zeros(\n",
    "        (len(encoded_corpus), seq_len),\n",
    "        dtype=np.int32\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(len(corpus)), desc=\"pad\"):\n",
    "        tokens = encoded_corpus[i][1]\n",
    "\n",
    "        # nltk's stopwords are a bit agrressive, ignore token lists with 0 size\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "\n",
    "        padded[i, -len(tokens):] = np.array(tokens)\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "pad: 100%|██████████| 64720/64720 [00:00<00:00, 389770.06it/s]\n"
     ]
    }
   ],
   "source": [
    "padded_tokens = pad_tokens(encoded_corpus)"
   ]
  },
  {
   "source": [
    "## Input and Labels?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_tokens # input\n",
    "y = np.array([c[0] for c in encoded_corpus])  #label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64720, 85)\n(64720,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42, train_size=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train = (41420, 85) # y_train = (41420,)\nx_val = (10356, 85) # y_val = (10356,)\nx_test = (12944, 85) # y_test = (12944,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train = {x_train.shape} # y_train = {y_train.shape}\")\n",
    "print(f\"x_val = {x_val.shape} # y_val = {y_val.shape}\")\n",
    "print(f\"x_test = {x_test.shape} # y_test = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to TensorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "val_data = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))\n",
    "test_data = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))"
   ]
  },
  {
   "source": [
    "## DataLoader for Torch\n",
    "\n",
    "Let torch handle the shuffling and etc yada stuff for MiniBatch\n",
    "\n",
    "Why MiniBatch? Dataset is big and feeding everything at once won't generalize well. (Even if the machine can handle it!)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# define a batch size\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(training_data, shuffle=True, batch_size=64)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=64)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd09b1e63f768c3f485360a541963d94d44221390383f2273abc7787eaf594e654f",
   "display_name": "Python 3.8.8 64-bit ('exp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}