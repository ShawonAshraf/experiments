{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "Polarity Dataset. Pang/Lee ACL 2004\n",
    "\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text file Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_path = \"./review_polarity/txt_sentoken/pos\"\n",
    "neg_path = \"./review_polarity/txt_sentoken/neg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_text_files(category_path):\n",
    "    file_list = os.listdir(category_path)\n",
    "    texts = []\n",
    "\n",
    "    for fname in file_list:\n",
    "        with open(os.path.join(category_path, fname), \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            texts.extend(lines)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "- tokenize\n",
    "- Clean punctuation, stopwords\n",
    "- Assign labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "ignore = stopwords.words(\"english\")\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    x = [token for token in tokens if token not in ignore]\n",
    "    return x\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    x = [token for token in tokens if token not in punctuation]\n",
    "    return x\n",
    "\n",
    "def clean(tokens):\n",
    "    #x = remove_stopwords(text)\n",
    "    x = remove_punctuation(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# label -> 1 for pos and 0 for neg\n",
    "def prepare_corpus():\n",
    "    # dataset is a list of tuples\n",
    "    # (label, tokens)\n",
    "    corpus = list()\n",
    "\n",
    "    # idx -> label\n",
    "    text_paths = [neg_path, pos_path]\n",
    "    for idx, path in enumerate(text_paths):\n",
    "        texts = read_text_files(path)\n",
    "        for i in tqdm(range(len(texts)), desc=\"prepare_corpus\"):\n",
    "            text = texts[i]\n",
    "            # tokenize\n",
    "            tokens = tokenize(text)\n",
    "            \n",
    "            # clean\n",
    "            tokens = remove_punctuation(tokens)\n",
    "            tokens = remove_stopwords(tokens)\n",
    "            \n",
    "            # append\n",
    "            corpus.append((idx, tokens))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "prepare_corpus: 100%|██████████| 31783/31783 [00:04<00:00, 6749.07it/s]\n",
      "prepare_corpus: 100%|██████████| 32937/32937 [00:05<00:00, 6573.49it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = prepare_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0,\n",
       " ['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive'])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "64720"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding - Word2Vec\n",
    "\n",
    "Google news embeddings this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/downloader.html\n",
    "\n",
    "import os\n",
    "import gensim.downloader as dl\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "pretrained_model_name = \"word2vec-google-news-300\"\n",
    "model_dl_path = os.path.join(dl.BASE_DIR, pretrained_model_name, f\"{pretrained_model_name}.gz\")\n",
    "\n",
    "if os.path.exists(model_dl_path):\n",
    "    # load model\n",
    "    corpus_embeddings = dl.load(pretrained_model_name)\n",
    "else:\n",
    "    # download\n",
    "    print(f\"Model will be downloaded at {model_dl_path}\")\n",
    "    corpus_embeddings = dl.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all the words word2vec has processed\n",
    "vocabulary = corpus_embeddings.index_to_key\n",
    "vocab_len = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode all tokens with indices from embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_corpus_tokens_with_embed_idx(corpus):\n",
    "    encoded_corpus = list()\n",
    "    for i in tqdm(range(len(corpus)), desc=\"encode_tokens_with_embed_idx\"):\n",
    "        idxs = []\n",
    "        label, tokens = corpus[i]\n",
    "\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                idx = corpus_embeddings.key_to_index[token]\n",
    "            except:\n",
    "                # if token isn't in the vocab\n",
    "                idx = 0\n",
    "\n",
    "            idxs.append(idx)\n",
    "        \n",
    "        \n",
    "        encoded_corpus.append((label, idxs))\n",
    "\n",
    "\n",
    "\n",
    "    return encoded_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "encode_tokens_with_embed_idx: 100%|██████████| 64720/64720 [00:00<00:00, 163687.16it/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_corpus = encode_corpus_tokens_with_embed_idx(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[4123, 54, 4154, 6717, 152, 1411, 447, 3554, 817]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "encoded_corpus[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "Left pad with 0\n",
    "\n",
    "However we need a sequence length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# get max sequences length\n",
    "sentences = [s[1] for s in corpus]\n",
    "max_seq_len = max(len(s) for s in sentences)\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_tokens(encoded_corpus, seq_len= 200):\n",
    "    padded = np.zeros(\n",
    "        (len(encoded_corpus), seq_len),\n",
    "        dtype=np.int32\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(len(corpus)), desc=\"pad\"):\n",
    "        tokens = encoded_corpus[i][1]\n",
    "\n",
    "        # nltk's stopwords are a bit agrressive, ignore token lists with 0 size\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "\n",
    "        padded[i, -len(tokens):] = np.array(tokens)\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "pad: 100%|██████████| 64720/64720 [00:00<00:00, 369833.21it/s]\n"
     ]
    }
   ],
   "source": [
    "padded_tokens = pad_tokens(encoded_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_tokens # input\n",
    "y = np.array([c[0] for c in encoded_corpus])  #label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64720, 200)\n(64720,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42, train_size=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train = (41420, 200) # y_train = (41420,)\nx_val = (10356, 200) # y_val = (10356,)\nx_test = (12944, 200) # y_test = (12944,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train = {x_train.shape} # y_train = {y_train.shape}\")\n",
    "print(f\"x_val = {x_val.shape} # y_val = {y_val.shape}\")\n",
    "print(f\"x_test = {x_test.shape} # y_test = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to TensorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "val_data = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader for Torch\n",
    "\n",
    "Let torch handle the shuffling and etc yada stuff for MiniBatch\n",
    "\n",
    "Why MiniBatch? Dataset is big and feeding everything at once won't generalize well. (Even if the machine can handle it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# define a batch size\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(training_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "\n",
    "This model is based on https://arxiv.org/abs/1408.5882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentClassifierCNN(nn.Module):\n",
    "    def __init__(self, freeze_embeddings=True):\n",
    "        super(SentimentClassifierCNN, self).__init__()\n",
    "        \n",
    "        # properties\n",
    "        self.kernel_sizes = [3,4,5]\n",
    "        self.num_filters = 100\n",
    "        self.embedding_dim = 300\n",
    "        self.output_size = 1\n",
    "        self.vocab_size=vocab_len\n",
    "\n",
    "        # convert embeddings to tensors!\n",
    "        self.corpus_embedding = torch.from_numpy(corpus_embeddings.vectors)\n",
    "\n",
    "        # neural network \n",
    "\n",
    "        # embedding layer\n",
    "        # by default we're freezing embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.corpus_embedding, freeze=freeze_embeddings)\n",
    "\n",
    "        # conv layers\n",
    "        # 3 conv layers, since 3 kernel sizes\n",
    "        self.conv1d = nn.ModuleList([\n",
    "            nn.Conv2d(1, self.num_filters, (k, self.embedding_dim), padding=(k - 2, 0))\n",
    "\n",
    "            for k in self.kernel_sizes\n",
    "        ])\n",
    "\n",
    "        # final linear layer\n",
    "        self.linear = nn.Linear(len(self.kernel_sizes) * self.num_filters, self.output_size)\n",
    "\n",
    "        # dropout and sigmoid\n",
    "        # why sigmoid? Well, binary classification task!\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # helper \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        \"\"\"\n",
    "        Convolutional + max pooling layer\n",
    "        \"\"\"\n",
    "        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        \n",
    "        # 1D pool over conv_seq_length\n",
    "        # squeeze to get size: (batch_size, num_filters)\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.unsqueeze(1) # reshape for conv (vector to matrix)\n",
    "\n",
    "        conv_out =  [self.conv_and_pool(embeds, conv) for conv in self.conv1d]\n",
    "\n",
    "        # concate convolution outputs as a \"vector\"\n",
    "        out = torch.cat(conv_out, 1)\n",
    "        # apply dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # linear \n",
    "        out = self.linear(out)\n",
    "\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SentimentClassifierCNN(\n  (embedding): Embedding(3000000, 300)\n  (conv1d): ModuleList(\n    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n  )\n  (linear): Linear(in_features=300, out_features=1, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (sigmoid): Sigmoid()\n)\n"
     ]
    }
   ],
   "source": [
    "cnn = SentimentClassifierCNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Device\n",
    "Using accelerate from huggingface\n",
    "https://huggingface.co/docs/accelerate/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add model, dataloader, optimizer and dataset to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn, train_loader, val_loader, optimizer = accelerator.prepare(\n",
    "    cnn, train_loader, val_loader, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train_cnn_for_2_epochs:   0%|          | 0/2 [00:00<?, ?it/s]Epoch: 1/2\tStep: 100\tTrain Loss: 0.6616680026054382\tValidation Loss: 0.6519948782829138\n",
      "Epoch: 1/2\tStep: 200\tTrain Loss: 0.656464695930481\tValidation Loss: 0.6416473127901554\n",
      "Epoch: 1/2\tStep: 300\tTrain Loss: 0.6641656160354614\tValidation Loss: 0.6364171649687566\n",
      "Epoch: 1/2\tStep: 400\tTrain Loss: 0.5873491764068604\tValidation Loss: 0.6347373434557364\n",
      "Epoch: 1/2\tStep: 500\tTrain Loss: 0.6204197406768799\tValidation Loss: 0.6329718263676534\n",
      "Epoch: 1/2\tStep: 600\tTrain Loss: 0.7505196332931519\tValidation Loss: 0.6394698854822379\n",
      "Epoch: 1/2\tStep: 700\tTrain Loss: 0.6436971426010132\tValidation Loss: 0.6307122068336377\n",
      "train_cnn_for_2_epochs:  50%|█████     | 1/2 [00:17<00:17, 17.49s/it]Epoch: 1/2\tStep: 800\tTrain Loss: 0.6809712648391724\tValidation Loss: 0.6301377673561757\n",
      "Epoch: 2/2\tStep: 900\tTrain Loss: 0.554826557636261\tValidation Loss: 0.6250122315608538\n",
      "Epoch: 2/2\tStep: 1000\tTrain Loss: 0.63131183385849\tValidation Loss: 0.6252176211430476\n",
      "Epoch: 2/2\tStep: 1100\tTrain Loss: 0.5207560062408447\tValidation Loss: 0.6274578077002213\n",
      "Epoch: 2/2\tStep: 1200\tTrain Loss: 0.6728614568710327\tValidation Loss: 0.6240252667608169\n",
      "Epoch: 2/2\tStep: 1300\tTrain Loss: 0.6059253215789795\tValidation Loss: 0.6249727495014668\n",
      "Epoch: 2/2\tStep: 1400\tTrain Loss: 0.5950549244880676\tValidation Loss: 0.6267401250795677\n",
      "Epoch: 2/2\tStep: 1500\tTrain Loss: 0.6500683426856995\tValidation Loss: 0.6309743393212557\n",
      "Epoch: 2/2\tStep: 1600\tTrain Loss: 0.4700145721435547\tValidation Loss: 0.6301737642632081\n",
      "train_cnn_for_2_epochs: 100%|██████████| 2/2 [00:32<00:00, 16.44s/it]\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "def train_cnn(model, train_loader, val_loader, epochs, optimizer, loss_fn, accl=accelerator):\n",
    "    print_counter = 0 # print loss for each 100th count\n",
    "\n",
    "    for e in tqdm(range(epochs), desc=f\"train_cnn_for_{epochs}_epochs\"):\n",
    "        model.train()\n",
    "        for input, label in train_loader:\n",
    "            print_counter += 1\n",
    "            # zero gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            output = model(input)\n",
    "\n",
    "            # backprop\n",
    "            loss = loss_fn(output.squeeze(), label.float())\n",
    "            accl.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            # log loss \n",
    "            if print_counter % 100 == 0:\n",
    "                validation_losses = []\n",
    "                \n",
    "                model.eval() # switch mode\n",
    "                with torch.no_grad():\n",
    "                    for val_input, val_label in val_loader:\n",
    "                        val_output = model(val_input)\n",
    "                        val_loss = loss_fn(val_output.squeeze(), val_label.float())\n",
    "                        validation_losses.append(val_loss.item())\n",
    "                    print(f\"Epoch: {e + 1}/{epochs}\\tStep: {print_counter}\\tTrain Loss: {loss.item()}\\tValidation Loss: {np.mean(validation_losses)}\")\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "train_cnn(\n",
    "    model=cnn,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=epochs,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_tensor = torch.from_numpy(x_test)\n",
    "test_x_tensor = test_x_tensor.to(device)\n",
    "\n",
    "def classify_sentiment(model, test_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        out = model(test_data)\n",
    "        out = torch.round(out.squeeze())\n",
    "    \t\n",
    "        return out.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 1., 1.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "y_pred = classify_sentiment(cnn, test_x_tensor)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.66      0.54      0.59      6328\n           1       0.62      0.74      0.68      6616\n\n    accuracy                           0.64     12944\n   macro avg       0.64      0.64      0.63     12944\nweighted avg       0.64      0.64      0.64     12944\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd09b1e63f768c3f485360a541963d94d44221390383f2273abc7787eaf594e654f",
   "display_name": "Python 3.8.8 64-bit ('exp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}