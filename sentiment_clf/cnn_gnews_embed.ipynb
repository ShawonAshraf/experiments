{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "Polarity Dataset. Pang/Lee ACL 2004\n",
    "\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
    "\n",
    "Based on : https://github.com/cezannec/CNN_Text_Classification/blob/master/CNN_Text_Classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "prepare_corpus: 100%|██████████| 1000/1000 [00:04<00:00, 224.65it/s]\n",
      "prepare_corpus: 100%|██████████| 1000/1000 [00:04<00:00, 210.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from corpus import prepare_corpus\n",
    "\n",
    "corpus = prepare_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding - Word2Vec\n",
    "\n",
    "Google news embeddings this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading model from C:\\Users\\shawo/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "# https://radimrehurek.com/gensim/downloader.html\n",
    "\n",
    "import os\n",
    "import gensim.downloader as dl\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "pretrained_model_name = \"word2vec-google-news-300\"\n",
    "model_dl_path = os.path.join(dl.BASE_DIR, pretrained_model_name, f\"{pretrained_model_name}.gz\")\n",
    "\n",
    "if os.path.exists(model_dl_path):\n",
    "    # load model\n",
    "    print(f\"Loading model from {model_dl_path}\")\n",
    "    gnews_embeddings = dl.load(pretrained_model_name)\n",
    "else:\n",
    "    # download\n",
    "    print(f\"Model will be downloaded at {model_dl_path}\")\n",
    "    corpus_embeddings = dl.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all the words word2vec has processed\n",
    "vocabulary = gnews_embeddings.index_to_key\n",
    "vocab_len = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode all tokens with indices from embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def encode_corpus_tokens_with_embed_idx(corpus):\n",
    "    encoded_corpus = list()\n",
    "    for i in tqdm(range(len(corpus)), desc=\"encode_tokens_with_embed_idx\"):\n",
    "        idxs = []\n",
    "        label, tokens = corpus[i]\n",
    "\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                idx = gnews_embeddings.key_to_index[token]\n",
    "            except:\n",
    "                # if token isn't in the vocab\n",
    "                idx = 0\n",
    "\n",
    "            idxs.append(idx)\n",
    "        \n",
    "        \n",
    "        encoded_corpus.append((label, idxs))\n",
    "\n",
    "\n",
    "\n",
    "    return encoded_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "encode_tokens_with_embed_idx: 100%|██████████| 2000/2000 [00:00<00:00, 7380.14it/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_corpus = encode_corpus_tokens_with_embed_idx(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "Left pad with 0\n",
    "\n",
    "However we need a sequence length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1477"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# get max sequences length\n",
    "sentences = [s[1] for s in corpus]\n",
    "max_seq_len = max(len(s) for s in sentences)\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_tokens(encoded_corpus, seq_len=max_seq_len):\n",
    "    padded = np.zeros(\n",
    "        (len(encoded_corpus), seq_len),\n",
    "        dtype=np.int32\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(len(corpus)), desc=\"pad\"):\n",
    "        tokens = encoded_corpus[i][1]\n",
    "\n",
    "        # nltk's stopwords are a bit agrressive, ignore token lists with 0 size\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "\n",
    "        padded[i, -len(tokens):] = np.array(tokens)\n",
    "\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "pad: 100%|██████████| 2000/2000 [00:00<00:00, 33333.89it/s]\n"
     ]
    }
   ],
   "source": [
    "padded_tokens = pad_tokens(encoded_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_tokens # input\n",
    "y = np.array([c[0] for c in encoded_corpus])  #label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2000, 1477)\n(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42, train_size=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train = (1280, 1477) # y_train = (1280,)\nx_val = (320, 1477) # y_val = (320,)\nx_test = (400, 1477) # y_test = (400,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train = {x_train.shape} # y_train = {y_train.shape}\")\n",
    "print(f\"x_val = {x_val.shape} # y_val = {y_val.shape}\")\n",
    "print(f\"x_test = {x_test.shape} # y_test = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to TensorData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "val_data = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader for Torch\n",
    "\n",
    "Let torch handle the shuffling and etc yada stuff for MiniBatch\n",
    "\n",
    "Why MiniBatch? Dataset is big and feeding everything at once won't generalize well. (Even if the machine can handle it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# define a batch size\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(training_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "\n",
    "This model is based on https://arxiv.org/abs/1408.5882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentClassifierCNN(nn.Module):\n",
    "    def __init__(self, freeze_embeddings=True):\n",
    "        super(SentimentClassifierCNN, self).__init__()\n",
    "        \n",
    "        # properties\n",
    "        self.kernel_sizes = [3,4,5]\n",
    "        self.num_filters = 100\n",
    "        self.embedding_dim = 300 # gnews300\n",
    "        self.output_size = 1\n",
    "        self.vocab_size=vocab_len\n",
    "\n",
    "        # convert embeddings to tensors!\n",
    "        self.embedding = torch.from_numpy(gnews_embeddings.vectors)\n",
    "\n",
    "        # neural network \n",
    "\n",
    "        # embedding layer\n",
    "        # by default we're freezing embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.embedding, freeze=freeze_embeddings)\n",
    "\n",
    "        # conv layers\n",
    "        # 3 conv layers, since 3 kernel sizes\n",
    "        self.conv1d = nn.ModuleList([\n",
    "            nn.Conv2d(1, self.num_filters, (k, self.embedding_dim), padding=(k - 2, 0))\n",
    "\n",
    "            for k in self.kernel_sizes\n",
    "        ])\n",
    "\n",
    "        # final linear layer\n",
    "        self.linear = nn.Linear(len(self.kernel_sizes) * self.num_filters, self.output_size)\n",
    "\n",
    "        # dropout and sigmoid\n",
    "        # why sigmoid? Well, binary classification task!\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # helper \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        \"\"\"\n",
    "        Convolutional + max pooling layer\n",
    "        \"\"\"\n",
    "        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        \n",
    "        # 1D pool over conv_seq_length\n",
    "        # squeeze to get size: (batch_size, num_filters)\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.unsqueeze(1) # reshape for conv (vector to matrix)\n",
    "\n",
    "        conv_out =  [self.conv_and_pool(embeds, conv) for conv in self.conv1d]\n",
    "\n",
    "        # concate convolution outputs as a \"vector\"\n",
    "        out = torch.cat(conv_out, 1)\n",
    "        # apply dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # linear \n",
    "        out = self.linear(out)\n",
    "\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SentimentClassifierCNN(\n  (embedding): Embedding(3000000, 300)\n  (conv1d): ModuleList(\n    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n  )\n  (linear): Linear(in_features=300, out_features=1, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (sigmoid): Sigmoid()\n)\n"
     ]
    }
   ],
   "source": [
    "cnn = SentimentClassifierCNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Device\n",
    "Using accelerate from huggingface\n",
    "https://huggingface.co/docs/accelerate/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add model, dataloader, optimizer and dataset to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn, train_loader, val_loader, optimizer = accelerator.prepare(\n",
    "    cnn, train_loader, val_loader, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "train_cnn_for_15_epochs:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Epoch: 1/15\tStep: 10\tTrain Loss: 0.6800340414047241\tValidation Loss: 0.6815965175628662\n",
      "\n",
      "Epoch: 1/15\tStep: 20\tTrain Loss: 0.6725341081619263\tValidation Loss: 0.685352052961077\n",
      "train_cnn_for_15_epochs:   7%|▋         | 1/15 [00:04<00:59,  4.27s/it]\n",
      "Epoch: 2/15\tStep: 30\tTrain Loss: 0.600382387638092\tValidation Loss: 0.6436454653739929\n",
      "\n",
      "Epoch: 2/15\tStep: 40\tTrain Loss: 0.6341875195503235\tValidation Loss: 0.6615862505776542\n",
      "train_cnn_for_15_epochs:  13%|█▎        | 2/15 [00:07<00:47,  3.62s/it]\n",
      "Epoch: 2/15\tStep: 50\tTrain Loss: 0.5865278840065002\tValidation Loss: 0.5992969189371381\n",
      "\n",
      "Epoch: 3/15\tStep: 60\tTrain Loss: 0.49015775322914124\tValidation Loss: 0.6082991872514997\n",
      "\n",
      "Epoch: 3/15\tStep: 70\tTrain Loss: 0.4470709264278412\tValidation Loss: 0.5463864292417254\n",
      "train_cnn_for_15_epochs:  20%|██        | 3/15 [00:10<00:40,  3.39s/it]\n",
      "Epoch: 4/15\tStep: 80\tTrain Loss: 0.41555091738700867\tValidation Loss: 0.5163954709257398\n",
      "\n",
      "Epoch: 4/15\tStep: 90\tTrain Loss: 0.35473760962486267\tValidation Loss: 0.5161402693816594\n",
      "\n",
      "Epoch: 4/15\tStep: 100\tTrain Loss: 0.4016858637332916\tValidation Loss: 0.4830940748964037\n",
      "train_cnn_for_15_epochs:  27%|██▋       | 4/15 [00:13<00:36,  3.32s/it]\n",
      "Epoch: 5/15\tStep: 110\tTrain Loss: 0.26962441205978394\tValidation Loss: 0.43878405008997234\n",
      "\n",
      "Epoch: 5/15\tStep: 120\tTrain Loss: 0.2384243905544281\tValidation Loss: 0.43588483333587646\n",
      "train_cnn_for_15_epochs:  33%|███▎      | 5/15 [00:17<00:33,  3.30s/it]\n",
      "Epoch: 5/15\tStep: 130\tTrain Loss: 0.230937197804451\tValidation Loss: 0.41474163106509615\n",
      "\n",
      "Epoch: 6/15\tStep: 140\tTrain Loss: 0.15032416582107544\tValidation Loss: 0.44769725629261564\n",
      "\n",
      "Epoch: 6/15\tStep: 150\tTrain Loss: 0.15078109502792358\tValidation Loss: 0.4013818417276655\n",
      "train_cnn_for_15_epochs:  40%|████      | 6/15 [00:19<00:28,  3.11s/it]\n",
      "Epoch: 7/15\tStep: 160\tTrain Loss: 0.10408297926187515\tValidation Loss: 0.3930243509156363\n",
      "\n",
      "Epoch: 7/15\tStep: 170\tTrain Loss: 0.12613698840141296\tValidation Loss: 0.385000262941633\n",
      "train_cnn_for_15_epochs:  47%|████▋     | 7/15 [00:22<00:25,  3.14s/it]\n",
      "Epoch: 7/15\tStep: 180\tTrain Loss: 0.11398592591285706\tValidation Loss: 0.4039532584803445\n",
      "\n",
      "Epoch: 8/15\tStep: 190\tTrain Loss: 0.08598985522985458\tValidation Loss: 0.3687205059187753\n",
      "\n",
      "Epoch: 8/15\tStep: 200\tTrain Loss: 0.0640333965420723\tValidation Loss: 0.3727620967796871\n",
      "train_cnn_for_15_epochs:  53%|█████▎    | 8/15 [00:25<00:21,  3.04s/it]\n",
      "Epoch: 9/15\tStep: 210\tTrain Loss: 0.05456777289509773\tValidation Loss: 0.3657893197877066\n",
      "\n",
      "Epoch: 9/15\tStep: 220\tTrain Loss: 0.04642143100500107\tValidation Loss: 0.38543687547956196\n",
      "\n",
      "Epoch: 9/15\tStep: 230\tTrain Loss: 0.05520299822092056\tValidation Loss: 0.3571491965225765\n",
      "train_cnn_for_15_epochs:  60%|██████    | 9/15 [00:29<00:18,  3.14s/it]\n",
      "Epoch: 10/15\tStep: 240\tTrain Loss: 0.040154580026865005\tValidation Loss: 0.3645139847482954\n",
      "\n",
      "Epoch: 10/15\tStep: 250\tTrain Loss: 0.039254117757081985\tValidation Loss: 0.3601423331669399\n",
      "train_cnn_for_15_epochs:  67%|██████▋   | 10/15 [00:32<00:15,  3.18s/it]\n",
      "Epoch: 10/15\tStep: 260\tTrain Loss: 0.045849356800317764\tValidation Loss: 0.3488892614841461\n",
      "\n",
      "Epoch: 11/15\tStep: 270\tTrain Loss: 0.041621387004852295\tValidation Loss: 0.3493979871273041\n",
      "\n",
      "Epoch: 11/15\tStep: 280\tTrain Loss: 0.02512759156525135\tValidation Loss: 0.366813314812524\n",
      "train_cnn_for_15_epochs:  73%|███████▎  | 11/15 [00:35<00:12,  3.06s/it]\n",
      "Epoch: 12/15\tStep: 290\tTrain Loss: 0.030726918950676918\tValidation Loss: 0.36199247411319185\n",
      "\n",
      "Epoch: 12/15\tStep: 300\tTrain Loss: 0.022996682673692703\tValidation Loss: 0.3476299600941794\n",
      "train_cnn_for_15_epochs:  80%|████████  | 12/15 [00:38<00:09,  3.10s/it]\n",
      "Epoch: 12/15\tStep: 310\tTrain Loss: 0.025151357054710388\tValidation Loss: 0.34760165001664844\n",
      "\n",
      "Epoch: 13/15\tStep: 320\tTrain Loss: 0.024328438565135002\tValidation Loss: 0.34049719146319796\n",
      "\n",
      "Epoch: 13/15\tStep: 330\tTrain Loss: 0.021412409842014313\tValidation Loss: 0.3577084243297577\n",
      "train_cnn_for_15_epochs:  87%|████████▋ | 13/15 [00:41<00:06,  3.01s/it]\n",
      "Epoch: 14/15\tStep: 340\tTrain Loss: 0.0191253200173378\tValidation Loss: 0.35531056353024076\n",
      "\n",
      "Epoch: 14/15\tStep: 350\tTrain Loss: 0.019895171746611595\tValidation Loss: 0.3471434329237257\n",
      "\n",
      "Epoch: 14/15\tStep: 360\tTrain Loss: 0.018732156604528427\tValidation Loss: 0.36155614256858826\n",
      "train_cnn_for_15_epochs:  93%|█████████▎| 14/15 [00:44<00:03,  3.13s/it]\n",
      "Epoch: 15/15\tStep: 370\tTrain Loss: 0.017626073211431503\tValidation Loss: 0.33927840845925467\n",
      "\n",
      "Epoch: 15/15\tStep: 380\tTrain Loss: 0.01754799857735634\tValidation Loss: 0.36854169837066103\n",
      "train_cnn_for_15_epochs: 100%|██████████| 15/15 [00:47<00:00,  3.20s/it]\n",
      "Epoch: 15/15\tStep: 390\tTrain Loss: 0.015580830164253712\tValidation Loss: 0.33798608609608244\n",
      "Wall time: 48 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "def train_cnn(model, train_loader, val_loader, epochs, optimizer, loss_fn, accl=accelerator):\n",
    "    print_counter = 0 # print loss for each 10th count\n",
    "\n",
    "    for e in tqdm(range(epochs), desc=f\"train_cnn_for_{epochs}_epochs\"):\n",
    "        model.train()\n",
    "        for input, label in train_loader:\n",
    "            print_counter += 1\n",
    "            # zero gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            output = model(input)\n",
    "\n",
    "            # backprop\n",
    "            loss = loss_fn(output.squeeze(), label.float())\n",
    "            accl.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            # log loss \n",
    "            if print_counter % 10 == 0:\n",
    "                validation_losses = []\n",
    "                \n",
    "                model.eval() # switch mode\n",
    "                with torch.no_grad():\n",
    "                    for val_input, val_label in val_loader:\n",
    "                        val_output = model(val_input)\n",
    "                        val_loss = loss_fn(val_output.squeeze(), val_label.float())\n",
    "                        validation_losses.append(val_loss.item())\n",
    "                    print(f\"\\nEpoch: {e + 1}/{epochs}\\tStep: {print_counter}\\tTrain Loss: {loss.item()}\\tValidation Loss: {np.mean(validation_losses)}\")\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "%time train_cnn(model=cnn, train_loader=train_loader, val_loader=val_loader, epochs=epochs, optimizer=optimizer, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_tensor = torch.from_numpy(x_test)\n",
    "test_x_tensor = test_x_tensor.to(device)\n",
    "\n",
    "def classify_sentiment(model, test_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        out = model(test_data)\n",
    "        out = torch.round(out.squeeze())\n",
    "    \t\n",
    "        return out.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classify_sentiment(cnn, test_x_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.84      0.84      0.84       199\n           1       0.84      0.84      0.84       201\n\n    accuracy                           0.84       400\n   macro avg       0.84      0.84      0.84       400\nweighted avg       0.84      0.84      0.84       400\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_pred=y_pred, y_true=y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd09b1e63f768c3f485360a541963d94d44221390383f2273abc7787eaf594e654f",
   "display_name": "Python 3.8.8 64-bit ('exp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}