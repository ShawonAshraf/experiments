{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as dl\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /home/shawon/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights_name = \"word2vec-google-news-300\"\n",
    "model_dl_path = os.path.join(\n",
    "    dl.BASE_DIR, pretrained_weights_name, f\"{pretrained_weights_name}.gz\")\n",
    "\n",
    "\n",
    "if os.path.exists(model_dl_path):\n",
    "    # load model\n",
    "    print(f\"Loading model from {model_dl_path}\")\n",
    "    gnews_embeddings = dl.load(pretrained_weights_name)\n",
    "else:\n",
    "    # download\n",
    "    print(f\"Model will be downloaded at {model_dl_path}\")\n",
    "    gnews_embeddings = dl.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = gnews_embeddings.index_to_key\n",
    "vocab_len = len(vocabulary)\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Oneplus/Tweebank\n",
    "\n",
    "train_file = os.path.join(\n",
    "    \"/mnt/Others/experiments/datasets/Tweebank-dev/converted/\"\n",
    "    \"en-ud-tweet-train.fixed.conllu\")\n",
    "\n",
    "# assert os.path.exists(train_file)\n",
    "\n",
    "with open(train_file) as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# tweet_id = feb_jul_16.1463316480\\n',\n",
       " \"# text = RT @USER991: Dear diary,       I've been rapping in 3 accents and no longer know which one is truly mine. I am a sadting - Drake URL217…\\n\",\n",
       " '1\\tRT\\trt\\tX\\t_\\t_\\t10\\tdiscourse\\t_\\t_\\n',\n",
       " '2\\t@USER991\\t@USER\\tX\\t_\\t_\\t1\\tdiscourse\\t_\\tSpaceAfter=No\\n',\n",
       " '3\\t:\\t:\\tPUNCT\\t_\\t_\\t1\\tpunct\\t_\\t_\\n',\n",
       " '4\\tDear\\tdear\\tADJ\\t_\\t_\\t5\\tamod\\t_\\t_\\n',\n",
       " '5\\tdiary\\tdiary\\tNOUN\\t_\\t_\\t10\\tvocative\\t_\\tSpaceAfter=No\\n',\n",
       " '6\\t,\\t,\\tPUNCT\\t_\\t_\\t10\\tpunct\\t_\\t_\\n',\n",
       " '7\\tI\\ti\\tPRON\\t_\\t_\\t10\\tnsubj\\t_\\tSpaceAfter=No\\n',\n",
       " \"8\\t've\\t've\\tAUX\\t_\\t_\\t10\\taux\\t_\\t_\\n\",\n",
       " '9\\tbeen\\tbe\\tAUX\\t_\\t_\\t10\\taux\\t_\\t_\\n',\n",
       " '10\\trapping\\trap\\tVERB\\t_\\t_\\t0\\troot\\t_\\t_\\n',\n",
       " '11\\tin\\tin\\tADP\\t_\\t_\\t13\\tcase\\t_\\t_\\n',\n",
       " '12\\t3\\tNUMBER\\tNUM\\t_\\t_\\t13\\tnummod\\t_\\t_\\n',\n",
       " '13\\taccents\\taccent\\tNOUN\\t_\\t_\\t10\\tobl\\t_\\t_\\n',\n",
       " '14\\tand\\tand\\tCCONJ\\t_\\t_\\t17\\tcc\\t_\\t_\\n',\n",
       " '15\\tno\\tno\\tADV\\t_\\t_\\t16\\tadvmod\\t_\\t_\\n',\n",
       " '16\\tlonger\\tlonger\\tADV\\t_\\t_\\t17\\tadvmod\\t_\\t_\\n',\n",
       " '17\\tknow\\tknow\\tVERB\\t_\\t_\\t10\\tconj\\t_\\t_\\n',\n",
       " '18\\twhich\\twhich\\tDET\\t_\\t_\\t19\\tdet\\t_\\t_\\n',\n",
       " '19\\tone\\tone\\tNUM\\t_\\t_\\t22\\tnsubj\\t_\\t_\\n',\n",
       " '20\\tis\\tbe\\tAUX\\t_\\t_\\t22\\tcop\\t_\\t_\\n',\n",
       " '21\\ttruly\\ttruly\\tADV\\t_\\t_\\t22\\tadvmod\\t_\\t_\\n',\n",
       " '22\\tmine\\tmine\\tPRON\\t_\\t_\\t17\\tccomp\\t_\\tSpaceAfter=No\\n',\n",
       " '23\\t.\\t.\\tPUNCT\\t_\\t_\\t10\\tpunct\\t_\\t_\\n',\n",
       " '24\\tI\\ti\\tPRON\\t_\\t_\\t27\\tnsubj\\t_\\t_\\n',\n",
       " '25\\tam\\tbe\\tAUX\\t_\\t_\\t27\\tcop\\t_\\t_\\n',\n",
       " '26\\ta\\ta\\tDET\\t_\\t_\\t27\\tdet\\t_\\t_\\n',\n",
       " '27\\tsadting\\tsadting\\tNOUN\\t_\\t_\\t10\\tparataxis\\t_\\t_\\n',\n",
       " '28\\t-\\t-\\tPUNCT\\t_\\t_\\t27\\tpunct\\t_\\t_\\n',\n",
       " '29\\tDrake\\tdrake\\tPROPN\\t_\\t_\\t27\\tparataxis\\t_\\t_\\n',\n",
       " '30\\tURL217\\tURL\\tX\\t_\\t_\\t27\\tlist\\t_\\tSpaceAfter=No\\n',\n",
       " '31\\t…\\t…\\tPUNCT\\t_\\t_\\t27\\tpunct\\t_\\tSpaceAfter=\\\\n\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break line at every \"\\n\"\n",
    "tweets = list()\n",
    "buffer = list()\n",
    "for idx, tw in enumerate(data):\n",
    "    if tw == \"\\n\":\n",
    "        # one partition here\n",
    "        tweets.append(buffer)\n",
    "        buffer = []\n",
    "    else:\n",
    "        # keep appending\n",
    "        buffer.append(tw)\n",
    "        \n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', 'Dear', 'dear', 'ADJ', '_', '_', '5', 'amod', '_', '_\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format for tokens\n",
    "# number - word - lemma - pos - _ - _ - id - role, -, - \n",
    "\n",
    "'4\\tDear\\tdear\\tADJ\\t_\\t_\\t5\\tamod\\t_\\t_\\n'.split(\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need idx 1, 2,3 : word, lemma and pos\n",
    "\n",
    "class ConlluRowInfo:\n",
    "    word: str\n",
    "    lemma: str\n",
    "    pos: str\n",
    "    \n",
    "    def __init__(self, word: str, lemma: str, pos: str) -> None:\n",
    "        self.word = word\n",
    "        self.lemma = lemma\n",
    "        self.pos = pos\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        rep = {\n",
    "            \"word\": self.word,\n",
    "            \"lemma\": self.lemma,\n",
    "            \"pos\": self.pos\n",
    "        }\n",
    "        return str(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class ConlluRow:\n",
    "    info: List[ConlluRowInfo]\n",
    "    # text: str\n",
    "    \n",
    "    def __init__(self, infos: List[ConlluRowInfo]) -> None:\n",
    "        self.info = infos\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        return f\"info : {self.info}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_tweets = list()\n",
    "\n",
    "for tweet in tweets:\n",
    "    # text = tweet[1].replace(\"# text = \", \"\")\n",
    "    info_in_tweet = list()\n",
    "    for infos in tweet[2:]:\n",
    "        buffer = infos.split(\"\\t\")\n",
    "        try:\n",
    "            word = buffer[1]\n",
    "            lemma = buffer[2]\n",
    "            tag = buffer[3]\n",
    "            info_in_tweet.append(ConlluRowInfo(word, lemma, tag))\n",
    "        except IndexError:\n",
    "            print(buffer)\n",
    "        except AttributeError as e:\n",
    "            print(e.name)\n",
    "    structured_tweets.append(ConlluRow(info_in_tweet))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to define the torch dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm._tqdm import trange, tqdm\n",
    "\n",
    "class TweebankDataset(Dataset):\n",
    "    def __init__(self, filename, w2v_weights=gnews_embeddings) -> None:\n",
    "        self.filename = filename\n",
    "        \n",
    "        self.w2v = w2v_weights\n",
    "        self.data = None\n",
    "        self.__read_data()\n",
    "        \n",
    "        # self.max_seq_len = 0\n",
    "        \n",
    "        self.UNIQUE_TAGS = ['PRON', 'NUM', 'NOUN', 'CCONJ', 'ADV', 'SCONJ', \n",
    "                               'ADP', 'AUX', 'PROPN', 'SYM', 'DET', \n",
    "                               'INTJ', 'PUNCT', 'X', 'ADJ', 'VERB', 'PART']\n",
    "        self.tag_dict = dict()\n",
    "        self.__encode_tags()\n",
    "        \n",
    "        self.number_tags = len(self.UNIQUE_TAGS)\n",
    "        \n",
    "        self.vocabulary = self.w2v.index_to_key  # type: ignore\n",
    "            \n",
    "    \n",
    "    def __len__(self) ->  int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # ============== collect ===================\n",
    "        words = [i.word for i in self.data[idx].info]\n",
    "        lemmas = [i.lemma for i in self.data[idx].info]\n",
    "        tags = [i.pos for i in self.data[idx].info]\n",
    "                \n",
    "        \n",
    "        # =================== convert using word2vec weights ==========\n",
    "        for idx in range(len(words)):\n",
    "            try:\n",
    "                w2v_idx = self.w2v.key_to_index[words[idx]]\n",
    "            except KeyError:\n",
    "                w2v_idx = 0\n",
    "            words[idx] = w2v_idx\n",
    "            tags[idx] = self.tag_dict[tags[idx]]\n",
    "        \n",
    "        return {\n",
    "            \"words\": words,\n",
    "            \"lemmas\": lemmas,\n",
    "            \"tags\": tags\n",
    "        }\n",
    "        \n",
    "    def __encode_tags(self):\n",
    "        for idx, tag in enumerate(self.UNIQUE_TAGS):\n",
    "            self.tag_dict[tag] = idx\n",
    "        \n",
    "    def __read_data(self):\n",
    "        with open(self.filename, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            \n",
    "            # ============ read the text file =============\n",
    "            lines = list()\n",
    "            buffer = list()\n",
    "            for _, line in tqdm(enumerate(data)):\n",
    "                if line == \"\\n\":\n",
    "                    lines.append(buffer)\n",
    "                    buffer = []\n",
    "                else:\n",
    "                    buffer.append(line)\n",
    "                    \n",
    "            # ============== organize in objects ==============\n",
    "            for idx, line in tqdm(enumerate(lines)):\n",
    "                # from line index 2 and onwards\n",
    "                line_info = list()\n",
    "                for info in line[2:]:\n",
    "                    buffer = info.split(\"\\t\")\n",
    "                \n",
    "                    try:\n",
    "                        word = buffer[1]\n",
    "                        lemma = buffer[2]\n",
    "                        tag = buffer[3]\n",
    "                        \n",
    "                        line_info.append(ConlluRowInfo(word, lemma, tag))\n",
    "                        \n",
    "                    except IndexError:\n",
    "                        print(buffer)\n",
    "                        \n",
    "                \n",
    "                lines[idx] = ConlluRow(line_info)    \n",
    "\n",
    "            self.data = lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29670it [00:00, 4080163.92it/s]\n",
      "1639it [00:00, 97075.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [31905, 0, 0, 12654, 14263, 0, 20, 190, 42, 40105, 1, 234, 22860, 0, 86, 951, 177, 48, 45, 4, 2604, 2747, 0, 20, 248, 0, 0, 0, 10297, 0, 0], 'lemmas': ['rt', '@USER', ':', 'dear', 'diary', ',', 'i', \"'ve\", 'be', 'rap', 'in', 'NUMBER', 'accent', 'and', 'no', 'longer', 'know', 'which', 'one', 'be', 'truly', 'mine', '.', 'i', 'be', 'a', 'sadting', '-', 'drake', 'URL', '…'], 'tags': [13, 13, 12, 14, 2, 12, 0, 7, 7, 15, 6, 1, 2, 3, 4, 4, 15, 10, 1, 7, 4, 0, 12, 0, 7, 10, 2, 12, 8, 13, 12]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = TweebankDataset(train_file)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'NUM', 'NOUN', 'CCONJ', 'ADV', 'SCONJ', 'ADP', 'AUX', 'PROPN', 'SYM', 'DET', 'INTJ', 'PUNCT', 'X', 'ADJ', 'VERB', 'PART']\n"
     ]
    }
   ],
   "source": [
    "# https://stackabuse.com/python-how-to-flatten-list-of-lists/\n",
    "\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# all_tags = [data[\"tags\"] for data in dataset]\n",
    "# all_tags = list(itertools.chain(*all_tags))\n",
    "# unique_tags = set(all_tags)\n",
    "# print(list(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('exp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5383723a85d76e4e9cac0d136e01f6d8a177dc9dc4ee2b9a35edd51227ec1b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
