{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as dl\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights_name = \"word2vec-google-news-300\"\n",
    "model_dl_path = os.path.join(\n",
    "    dl.BASE_DIR, pretrained_weights_name, f\"{pretrained_weights_name}.gz\")\n",
    "\n",
    "\n",
    "if os.path.exists(model_dl_path):\n",
    "    # load model\n",
    "    print(f\"Loading model from {model_dl_path}\")\n",
    "    gnews_embeddings = dl.load(pretrained_weights_name)\n",
    "else:\n",
    "    # download\n",
    "    print(f\"Model will be downloaded at {model_dl_path}\")\n",
    "    gnews_embeddings = dl.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = gnews_embeddings.index_to_key  # type: ignore\n",
    "vocab_len = len(vocabulary)\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Oneplus/Tweebank\n",
    "\n",
    "train_file = os.path.join(\n",
    "    \"/mnt/Others/experiments/datasets/Tweebank-dev/converted/\"\n",
    "    \"en-ud-tweet-train.fixed.conllu\")\n",
    "\n",
    "# assert os.path.exists(train_file)\n",
    "\n",
    "with open(train_file) as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break line at every \"\\n\"\n",
    "tweets = list()\n",
    "buffer = list()\n",
    "for idx, tw in enumerate(data):\n",
    "    if tw == \"\\n\":\n",
    "        # one partition here\n",
    "        tweets.append(buffer)\n",
    "        buffer = []\n",
    "    else:\n",
    "        # keep appending\n",
    "        buffer.append(tw)\n",
    "        \n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for tokens\n",
    "# number - word - lemma - pos - _ - _ - id - role, -, - \n",
    "\n",
    "'4\\tDear\\tdear\\tADJ\\t_\\t_\\t5\\tamod\\t_\\t_\\n'.split(\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need idx 1, 2,3 : word, lemma and pos\n",
    "\n",
    "class ConlluRowInfo:\n",
    "    word: str\n",
    "    lemma: str\n",
    "    pos: str\n",
    "    \n",
    "    def __init__(self, word: str, lemma: str, pos: str) -> None:\n",
    "        self.word = word\n",
    "        self.lemma = lemma\n",
    "        self.pos = pos\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        rep = {\n",
    "            \"word\": self.word,\n",
    "            \"lemma\": self.lemma,\n",
    "            \"pos\": self.pos\n",
    "        }\n",
    "        return str(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class ConlluRow:\n",
    "    info: List[ConlluRowInfo]\n",
    "    # text: str\n",
    "    \n",
    "    def __init__(self, infos: List[ConlluRowInfo]) -> None:\n",
    "        self.info = infos\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        return f\"info : {self.info}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_tweets = list()\n",
    "\n",
    "for tweet in tweets:\n",
    "    # text = tweet[1].replace(\"# text = \", \"\")\n",
    "    info_in_tweet = list()\n",
    "    for infos in tweet[2:]:\n",
    "        buffer = infos.split(\"\\t\")\n",
    "        try:\n",
    "            word = buffer[1]\n",
    "            lemma = buffer[2]\n",
    "            tag = buffer[3]\n",
    "            info_in_tweet.append(ConlluRowInfo(word, lemma, tag))\n",
    "        except IndexError:\n",
    "            print(buffer)\n",
    "        except AttributeError as e:\n",
    "            print(e.name)\n",
    "    structured_tweets.append(ConlluRow(info_in_tweet))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to define the torch dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import trange, tqdm\n",
    "from typing import Dict, List\n",
    "\n",
    "class TweebankDataset(Dataset):\n",
    "    def __init__(self, filename, w2v_weights=gnews_embeddings) -> None:\n",
    "        self.filename = filename\n",
    "        \n",
    "        self.w2v = w2v_weights\n",
    "        self.data = list()\n",
    "        self.__read_data()\n",
    "        \n",
    "        self.MAX_SEQ_LEN = 50 # default value\n",
    "        # self.__find_max_seq_len()\n",
    "        \n",
    "        self.UNIQUE_TAGS = ['PRON', 'NUM', 'NOUN', 'CCONJ', 'ADV', 'SCONJ', \n",
    "                               'ADP', 'AUX', 'PROPN', 'SYM', 'DET', \n",
    "                               'INTJ', 'PUNCT', 'X', 'ADJ', 'VERB', 'PART']\n",
    "        self.tag_dict = dict()\n",
    "        self.__encode_tags()\n",
    "        \n",
    "        self.number_tags = len(self.UNIQUE_TAGS)\n",
    "        \n",
    "        self.vocabulary = self.w2v.index_to_key  # type: ignore\n",
    "            \n",
    "    \n",
    "    def __len__(self) ->  int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        # ============== collect ===================\n",
    "        words = [i.word for i in self.data[idx].info]\n",
    "        # lemmas = [i.lemma for i in self.data[idx].info]\n",
    "        tags = [i.pos for i in self.data[idx].info]\n",
    "        \n",
    "        \n",
    "        # =================== convert using word2vec weights ==========\n",
    "        for idx in range(len(words)):\n",
    "            try:\n",
    "                w2v_idx = self.w2v.key_to_index[words[idx]]  # type: ignore \n",
    "            except KeyError:\n",
    "                w2v_idx = 0 # </s>\n",
    "            words[idx] = w2v_idx\n",
    "            tags[idx] = self.tag_dict[tags[idx]]\n",
    "            \n",
    "        \n",
    "        # ============== pad words ===============\n",
    "        # left pad\n",
    "        padded_words = np.zeros(self.MAX_SEQ_LEN, dtype=np.int32)\n",
    "        padded_words[-len(words):] = words\n",
    "        \n",
    "        padded_tags = np.zeros(self.MAX_SEQ_LEN, dtype=np.int32)\n",
    "        padded_tags[-len(tags):] = tags\n",
    "        \n",
    "        return {\n",
    "            \"words\": torch.tensor(padded_words),\n",
    "            # \"lemmas\": lemmas,\n",
    "            \"tags\": torch.tensor(padded_tags),\n",
    "            \"non_padded_len\": torch.tensor(len(tags))\n",
    "        }\n",
    "        \n",
    "    def __find_max_seq_len(self) -> None:\n",
    "        seq_lens = []\n",
    "        \n",
    "        for idx in range(len(self.data)):\n",
    "            words = [i.word for i in self.data[idx].info]\n",
    "            seq_lens.append(len(words))\n",
    "        \n",
    "        \n",
    "        self.MAX_SEQ_LEN = max(seq_lens)\n",
    "        \n",
    "    def __encode_tags(self) -> None:\n",
    "        for idx, tag in enumerate(self.UNIQUE_TAGS):\n",
    "            self.tag_dict[tag] = idx\n",
    "        \n",
    "    def __read_data(self) -> None:\n",
    "        with open(self.filename, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            \n",
    "            # ============ read the text file =============\n",
    "            lines = list()\n",
    "            buffer = list()\n",
    "            for _, line in tqdm(enumerate(data)):\n",
    "                if line == \"\\n\":\n",
    "                    lines.append(buffer)\n",
    "                    buffer = []\n",
    "                else:\n",
    "                    buffer.append(line)\n",
    "                    \n",
    "            # ============== organize in objects ==============\n",
    "            for idx, line in tqdm(enumerate(lines)):\n",
    "                # from line index 2 and onwards\n",
    "                line_info = list()\n",
    "                for info in line[2:]:\n",
    "                    buffer = info.split(\"\\t\")\n",
    "                \n",
    "                    try:\n",
    "                        word = buffer[1]\n",
    "                        lemma = buffer[2]\n",
    "                        tag = buffer[3]\n",
    "                        \n",
    "                        line_info.append(ConlluRowInfo(word, lemma, tag))\n",
    "                        \n",
    "                    except IndexError:\n",
    "                        print(buffer)\n",
    "                        \n",
    "                \n",
    "                lines[idx] = ConlluRow(line_info)    \n",
    "\n",
    "            self.data = lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TweebankDataset(train_file)\n",
    "sample = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/python-how-to-flatten-list-of-lists/\n",
    "\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# all_tags = [data[\"tags\"] for data in dataset]\n",
    "# all_tags = list(itertools.chain(*all_tags))\n",
    "# unique_tags = set(all_tags)\n",
    "# print(list(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_dim: int,  tagset_size: int, freeze_embeddings=True, w2v_weights=gnews_embeddings) -> None:\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.taget_size = tagset_size\n",
    "        self.freeze_embeddings = freeze_embeddings\n",
    "        # self.save_hyperparameters() \n",
    "        \n",
    "        \n",
    "        embedding_tensors = torch.from_numpy(w2v_weights.vectors) # type: ignore        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(\n",
    "            embedding_tensors, freeze=self.freeze_embeddings, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "        self.attention =  nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=0.1, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, words):\n",
    "        embeds = self.word_embeddings(words)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        linear_out = self.linear(attn_out)\n",
    "        linear_out = self.relu(linear_out)\n",
    "        logits = F.log_softmax(linear_out, dim=-1)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(embedding_dim=300, hidden_dim=100,  tagset_size=dataset.MAX_SEQ_LEN)\n",
    "\n",
    "# run a sample forward pass\n",
    "sample = dataset[42]\n",
    "out = model(sample[\"words\"])\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(out, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bs = 128\n",
    "dl_args = {\n",
    "    \"pin_memory\": True,\n",
    "    \"batch_size\": bs\n",
    "}\n",
    "\n",
    "\n",
    "training_set = dataset\n",
    "validation_set = TweebankDataset(\"/mnt/Others/experiments/datasets/Tweebank-dev/converted/en-ud-tweet-dev.fixed.conllu\")\n",
    "test_set = TweebankDataset(\"/mnt/Others/experiments/datasets/Tweebank-dev/converted/en-ud-tweet-test.fixed.conllu\")\n",
    "\n",
    "train_loader = DataLoader(training_set, shuffle=True, **dl_args)\n",
    "val_loader = DataLoader(validation_set, shuffle=False, **dl_args)\n",
    "test_loader = DataLoader(test_set, shuffle=False, **dl_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(params=model.parameters())\n",
    "criterion = nn.NLLLoss(ignore_index=0)\n",
    "run_validation_every_n_step = 100\n",
    "\n",
    "\n",
    "# fp16\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs = 300\n",
    "for e in trange(epochs):\n",
    "\n",
    "    steps = 0\n",
    "    for batch in train_loader:\n",
    "        # switch to train mode\n",
    "        model.train()\n",
    "        \n",
    "        words = batch[\"words\"]\n",
    "        tags = batch[\"tags\"].long()\n",
    "        \n",
    "        # send data to device\n",
    "        words = words.to(device)\n",
    "        tags = tags.to(device)\n",
    "        \n",
    "        # zero out optimizer to accumulate new grads\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            logits = model(words)\n",
    "            # loss\n",
    "            loss = criterion(logits, tags)\n",
    "        \n",
    "        \n",
    "        # ======== validation ==============\n",
    "        if steps % run_validation_every_n_step == 0:\n",
    "            val_losses = []\n",
    "            \n",
    "            # switch context\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    words = val_batch[\"words\"]\n",
    "                    tags = val_batch[\"tags\"].long()\n",
    "                    \n",
    "                    words = words.to(device)\n",
    "                    tags = tags.to(device)\n",
    "                    \n",
    "                    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                        logits = model(words)\n",
    "                        val_loss = criterion(logits, tags)\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                    # preds = torch.max(logits, dim=-1).indices\n",
    "\n",
    "                # log\n",
    "                print(f\"Epoch:: [{e + 1}]/[{epochs}] Step:: {steps}\")\n",
    "                print(f\"Train Loss:: {loss} __________ Val Loss:: {torch.mean(torch.tensor(val_losses))}\")\n",
    "        \n",
    "        # switch context\n",
    "        model.train()\n",
    "        scaler.scale(loss).backward()  # type: ignore\n",
    "        # loss.backward()\n",
    "        scaler.step(optimizer)\n",
    "        # optimizer.step()\n",
    "        scaler.update()\n",
    "        steps += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred:torch.Tensor, true: torch.Tensor, non_pad_start: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    \n",
    "    non_padded_pred = (pred != torch.tensor(0)).nonzero()\n",
    "    non_padded_true = (true != torch.tensor(0)).nonzero()\n",
    "    \n",
    "    correct = pred[non_padded_pred].eq(true[non_padded_true])\n",
    "    return correct.sum() / true.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        words = batch[\"words\"].to(device)\n",
    "        tags = batch[\"tags\"].long().to(device)\n",
    "        non_pad_start = batch[\"non_padded_len\"].to(device)\n",
    "        \n",
    "        \n",
    "        logits = model(words)\n",
    "        \n",
    "        preds = torch.max(logits, dim=0).indices\n",
    "        \n",
    "        a = accuracy(preds, tags, non_pad_start)\n",
    "#         print(a)\n",
    "        \n",
    "        acc.append(a)\n",
    "\n",
    "print(f\"Mean acc : {torch.mean(torch.tensor(acc))}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "5383723a85d76e4e9cac0d136e01f6d8a177dc9dc4ee2b9a35edd51227ec1b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
