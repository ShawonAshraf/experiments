{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-25T21:39:59.080877Z",
     "start_time": "2022-12-25T21:39:55.576076Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /home/shawon/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as dl\n",
    "\n",
    "pretrained_weights_name = \"word2vec-google-news-300\"\n",
    "model_dl_path = os.path.join(\n",
    "    dl.BASE_DIR, pretrained_weights_name, f\"{pretrained_weights_name}.gz\")\n",
    "\n",
    "\n",
    "if os.path.exists(model_dl_path):\n",
    "    # load model\n",
    "    print(f\"Loading model from {model_dl_path}\")\n",
    "    gnews_embeddings = dl.load(pretrained_weights_name)\n",
    "else:\n",
    "    # download\n",
    "    print(f\"Model will be downloaded at {model_dl_path}\")\n",
    "    gnews_embeddings = dl.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.575Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawon/anaconda3/envs/exp/lib/python3.10/site-packages/gensim/models/keyedvectors.py:552: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# add PAD to embeddings\n",
    "\n",
    "# 0 padding, 300 embedding dims\n",
    "gnews_embeddings.add_vector('<PAD>', np.zeros(300)) # type: ignore\n",
    "gnews_embeddings.add_vector('<OOV>', np.ones(300) * -1)  # type: ignore\n",
    "\n",
    "# need it later for loading the embeddings in pytorch model\n",
    "padding_idx = gnews_embeddings.get_index(\"<PAD>\") # type: ignore\n",
    "oov_idx = gnews_embeddings.get_index(\"<OOV>\") # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n",
      "3000001\n"
     ]
    }
   ],
   "source": [
    "print(padding_idx)\n",
    "print(oov_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.576Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/Oneplus/Tweebank\n",
    "\n",
    "train_file = os.path.join(\n",
    "    \"/mnt/Others/experiments/datasets/Tweebank-dev/converted/\"\n",
    "    \"en-ud-tweet-train.fixed.conllu\")\n",
    "\n",
    "# assert os.path.exists(train_file)\n",
    "\n",
    "with open(train_file) as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# tweet_id = feb_jul_16.1463316480\\n',\n",
       " \"# text = RT @USER991: Dear diary,       I've been rapping in 3 accents and no longer know which one is truly mine. I am a sadting - Drake URL217…\\n\",\n",
       " '1\\tRT\\trt\\tX\\t_\\t_\\t10\\tdiscourse\\t_\\t_\\n',\n",
       " '2\\t@USER991\\t@USER\\tX\\t_\\t_\\t1\\tdiscourse\\t_\\tSpaceAfter=No\\n',\n",
       " '3\\t:\\t:\\tPUNCT\\t_\\t_\\t1\\tpunct\\t_\\t_\\n',\n",
       " '4\\tDear\\tdear\\tADJ\\t_\\t_\\t5\\tamod\\t_\\t_\\n',\n",
       " '5\\tdiary\\tdiary\\tNOUN\\t_\\t_\\t10\\tvocative\\t_\\tSpaceAfter=No\\n',\n",
       " '6\\t,\\t,\\tPUNCT\\t_\\t_\\t10\\tpunct\\t_\\t_\\n',\n",
       " '7\\tI\\ti\\tPRON\\t_\\t_\\t10\\tnsubj\\t_\\tSpaceAfter=No\\n',\n",
       " \"8\\t've\\t've\\tAUX\\t_\\t_\\t10\\taux\\t_\\t_\\n\",\n",
       " '9\\tbeen\\tbe\\tAUX\\t_\\t_\\t10\\taux\\t_\\t_\\n',\n",
       " '10\\trapping\\trap\\tVERB\\t_\\t_\\t0\\troot\\t_\\t_\\n',\n",
       " '11\\tin\\tin\\tADP\\t_\\t_\\t13\\tcase\\t_\\t_\\n',\n",
       " '12\\t3\\tNUMBER\\tNUM\\t_\\t_\\t13\\tnummod\\t_\\t_\\n',\n",
       " '13\\taccents\\taccent\\tNOUN\\t_\\t_\\t10\\tobl\\t_\\t_\\n',\n",
       " '14\\tand\\tand\\tCCONJ\\t_\\t_\\t17\\tcc\\t_\\t_\\n',\n",
       " '15\\tno\\tno\\tADV\\t_\\t_\\t16\\tadvmod\\t_\\t_\\n',\n",
       " '16\\tlonger\\tlonger\\tADV\\t_\\t_\\t17\\tadvmod\\t_\\t_\\n',\n",
       " '17\\tknow\\tknow\\tVERB\\t_\\t_\\t10\\tconj\\t_\\t_\\n',\n",
       " '18\\twhich\\twhich\\tDET\\t_\\t_\\t19\\tdet\\t_\\t_\\n',\n",
       " '19\\tone\\tone\\tNUM\\t_\\t_\\t22\\tnsubj\\t_\\t_\\n',\n",
       " '20\\tis\\tbe\\tAUX\\t_\\t_\\t22\\tcop\\t_\\t_\\n',\n",
       " '21\\ttruly\\ttruly\\tADV\\t_\\t_\\t22\\tadvmod\\t_\\t_\\n',\n",
       " '22\\tmine\\tmine\\tPRON\\t_\\t_\\t17\\tccomp\\t_\\tSpaceAfter=No\\n',\n",
       " '23\\t.\\t.\\tPUNCT\\t_\\t_\\t10\\tpunct\\t_\\t_\\n',\n",
       " '24\\tI\\ti\\tPRON\\t_\\t_\\t27\\tnsubj\\t_\\t_\\n',\n",
       " '25\\tam\\tbe\\tAUX\\t_\\t_\\t27\\tcop\\t_\\t_\\n',\n",
       " '26\\ta\\ta\\tDET\\t_\\t_\\t27\\tdet\\t_\\t_\\n',\n",
       " '27\\tsadting\\tsadting\\tNOUN\\t_\\t_\\t10\\tparataxis\\t_\\t_\\n',\n",
       " '28\\t-\\t-\\tPUNCT\\t_\\t_\\t27\\tpunct\\t_\\t_\\n',\n",
       " '29\\tDrake\\tdrake\\tPROPN\\t_\\t_\\t27\\tparataxis\\t_\\t_\\n',\n",
       " '30\\tURL217\\tURL\\tX\\t_\\t_\\t27\\tlist\\t_\\tSpaceAfter=No\\n',\n",
       " '31\\t…\\t…\\tPUNCT\\t_\\t_\\t27\\tpunct\\t_\\tSpaceAfter=\\\\n\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break line at every \"\\n\"\n",
    "tweets = list()\n",
    "buffer = list()\n",
    "for idx, tw in enumerate(data):\n",
    "    if tw == \"\\n\":\n",
    "        # one partition here\n",
    "        tweets.append(buffer)\n",
    "        buffer = []\n",
    "    else:\n",
    "        # keep appending\n",
    "        buffer.append(tw)\n",
    "        \n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', 'Dear', 'dear', 'ADJ', '_', '_', '5', 'amod', '_', '_\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format for tokens\n",
    "# number - word - lemma - pos - _ - _ - id - role, -, - \n",
    "\n",
    "'4\\tDear\\tdear\\tADJ\\t_\\t_\\t5\\tamod\\t_\\t_\\n'.split(\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.578Z"
    }
   },
   "outputs": [],
   "source": [
    "# need idx 1, 2,3 : word, lemma and pos\n",
    "\n",
    "class ConlluRowInfo:\n",
    "    word: str\n",
    "    lemma: str\n",
    "    pos: str\n",
    "    \n",
    "    def __init__(self, word: str, lemma: str, pos: str) -> None:\n",
    "        self.word = word\n",
    "        self.lemma = lemma\n",
    "        self.pos = pos\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        rep = {\n",
    "            \"word\": self.word,\n",
    "            \"lemma\": self.lemma,\n",
    "            \"pos\": self.pos\n",
    "        }\n",
    "        return str(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.579Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class ConlluRow:\n",
    "    info: List[ConlluRowInfo]\n",
    "    # text: str\n",
    "    \n",
    "    def __init__(self, infos: List[ConlluRowInfo]) -> None:\n",
    "        self.info = infos\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        return f\"info : {self.info}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.579Z"
    }
   },
   "outputs": [],
   "source": [
    "structured_tweets = list()\n",
    "\n",
    "for tweet in tweets:\n",
    "    # text = tweet[1].replace(\"# text = \", \"\")\n",
    "    info_in_tweet = list()\n",
    "    # start from idx 2\n",
    "    for infos in tweet[2:]:\n",
    "        buffer = infos.split(\"\\t\")\n",
    "        try:\n",
    "            word = buffer[1]\n",
    "            lemma = buffer[2]\n",
    "            tag = buffer[3]\n",
    "            info_in_tweet.append(ConlluRowInfo(word, lemma, tag))\n",
    "        except IndexError:\n",
    "            print(buffer)\n",
    "        except AttributeError as e:\n",
    "            print(e.name)\n",
    "    structured_tweets.append(ConlluRow(info_in_tweet))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.8046875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(structured_tweets) / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.580Z"
    }
   },
   "outputs": [],
   "source": [
    "# time to define the torch dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import trange, tqdm\n",
    "from typing import Dict, List\n",
    "\n",
    "class TweebankDataset(Dataset):\n",
    "    def __init__(self, filename, oov_idx=oov_idx, padding_idx=padding_idx, w2v_weights=gnews_embeddings) -> None:\n",
    "        self.filename = filename\n",
    "        \n",
    "        self.w2v = w2v_weights\n",
    "        self.oov_idx = oov_idx\n",
    "        self.padding_idx = padding_idx\n",
    "        self.data = list()\n",
    "        self.__read_data()\n",
    "        \n",
    "        self.MAX_SEQ_LEN = 50 # default value\n",
    "        # self.__find_max_seq_len()\n",
    "        \n",
    "        self.UNIQUE_TAGS = ['PRON', 'NUM', 'NOUN', 'CCONJ', 'ADV', 'SCONJ', \n",
    "                               'ADP', 'AUX', 'PROPN', 'SYM', 'DET', \n",
    "                               'INTJ', 'PUNCT', 'X', 'ADJ', 'VERB', 'PART', '<PAD>']\n",
    "        self.tag_dict = dict()\n",
    "        self.__encode_tags()\n",
    "        \n",
    "        self.number_tags = len(self.UNIQUE_TAGS)\n",
    "        \n",
    "        self.vocabulary = self.w2v.index_to_key  # type: ignore\n",
    "            \n",
    "    \n",
    "    def __len__(self) ->  int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        # ============== collect ===================\n",
    "        words = [i.word for i in self.data[idx].info]\n",
    "        # lemmas = [i.lemma for i in self.data[idx].info]\n",
    "        tags = [i.pos for i in self.data[idx].info]\n",
    "        \n",
    "        \n",
    "        # =================== convert using word2vec weights ==========\n",
    "        for idx in range(len(words)):\n",
    "            try:\n",
    "                w2v_idx = self.w2v.key_to_index[words[idx]]  # type: ignore \n",
    "            except KeyError:\n",
    "                # indicates OOV\n",
    "                w2v_idx = self.oov_idx\n",
    "            words[idx] = w2v_idx\n",
    "            tags[idx] = self.tag_dict[tags[idx]]\n",
    "            \n",
    "        \n",
    "        # ============== pad words ===============\n",
    "        # left pad\n",
    "        padded_words = np.ones(self.MAX_SEQ_LEN, dtype=np.int32) * self.padding_idx\n",
    "        padded_words[-len(words):] = words\n",
    "        \n",
    "        # ============== pad tags =================\n",
    "        padded_tags = np.ones(self.MAX_SEQ_LEN, dtype=np.int32) * self.tag_dict.get(\"<PAD>\")  # type: ignore        \n",
    "        padded_tags[-len(tags):] = tags\n",
    "        \n",
    "        return {\n",
    "            \"words\": torch.tensor(padded_words),\n",
    "            \"tags\": torch.tensor(padded_tags),\n",
    "        }\n",
    "        \n",
    "    def __find_max_seq_len(self) -> None:\n",
    "        seq_lens = []\n",
    "        \n",
    "        for idx in range(len(self.data)):\n",
    "            words = [i.word for i in self.data[idx].info]\n",
    "            seq_lens.append(len(words))\n",
    "        \n",
    "        \n",
    "        self.MAX_SEQ_LEN = max(seq_lens)\n",
    "        \n",
    "    def __encode_tags(self) -> None:\n",
    "        for idx, tag in enumerate(self.UNIQUE_TAGS):\n",
    "            self.tag_dict[tag] = idx\n",
    "        \n",
    "    def __read_data(self) -> None:\n",
    "        with open(self.filename, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            \n",
    "            # ============ read the text file =============\n",
    "            lines = list()\n",
    "            buffer = list()\n",
    "            for _, line in tqdm(enumerate(data)):\n",
    "                if line == \"\\n\":\n",
    "                    lines.append(buffer)\n",
    "                    buffer = []\n",
    "                else:\n",
    "                    buffer.append(line)\n",
    "                    \n",
    "            # ============== organize in objects ==============\n",
    "            for idx, line in tqdm(enumerate(lines)):\n",
    "                # from line index 2 and onwards\n",
    "                line_info = list()\n",
    "                for info in line[2:]:\n",
    "                    buffer = info.split(\"\\t\")\n",
    "                \n",
    "                    try:\n",
    "                        word = buffer[1]\n",
    "                        lemma = buffer[2]\n",
    "                        tag = buffer[3]\n",
    "                        \n",
    "                        line_info.append(ConlluRowInfo(word, lemma, tag))\n",
    "                        \n",
    "                    except IndexError:\n",
    "                        print(buffer)\n",
    "                        \n",
    "                \n",
    "                lines[idx] = ConlluRow(line_info)    \n",
    "\n",
    "            self.data = lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c7f585c8e94e7e9840a3a229058037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0b55d588634df2b14bbf8bbdfdda05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'words': tensor([3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000,\n",
       "         3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000,\n",
       "         3000000, 3000000, 3000000,   31905, 3000001, 3000001,   12654,   14263,\n",
       "         3000001,      20,     190,      42,   40105,       1,     234,   22860,\n",
       "         3000001,      86,     951,     177,      48,      45,       4,    2604,\n",
       "            2747, 3000001,      20,     248, 3000001, 3000001, 3000001,   10297,\n",
       "         3000001, 3000001], dtype=torch.int32),\n",
       " 'tags': tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "         17, 13, 13, 12, 14,  2, 12,  0,  7,  7, 15,  6,  1,  2,  3,  4,  4, 15,\n",
       "         10,  1,  7,  4,  0, 12,  0,  7, 10,  2, 12,  8, 13, 12],\n",
       "        dtype=torch.int32)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TweebankDataset(train_file)\n",
    "sample = dataset[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRON': 0,\n",
       " 'NUM': 1,\n",
       " 'NOUN': 2,\n",
       " 'CCONJ': 3,\n",
       " 'ADV': 4,\n",
       " 'SCONJ': 5,\n",
       " 'ADP': 6,\n",
       " 'AUX': 7,\n",
       " 'PROPN': 8,\n",
       " 'SYM': 9,\n",
       " 'DET': 10,\n",
       " 'INTJ': 11,\n",
       " 'PUNCT': 12,\n",
       " 'X': 13,\n",
       " 'ADJ': 14,\n",
       " 'VERB': 15,\n",
       " 'PART': 16,\n",
       " '<PAD>': 17}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.582Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackabuse.com/python-how-to-flatten-list-of-lists/\n",
    "\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# all_tags = [data[\"tags\"] for data in dataset]\n",
    "# all_tags = list(itertools.chain(*all_tags))\n",
    "# unique_tags = set(all_tags)\n",
    "# print(list(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f0e99a8c994fbaa28b5a677a11ba58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ec8745a3644eceb24f4c79732944df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1913df1dfe346d0b3d7084e5cdc89e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8b37b6107148e792ad0e12e6403740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bs = 128\n",
    "dl_args = {\n",
    "    \"pin_memory\": True,\n",
    "    \"batch_size\": bs\n",
    "}\n",
    "\n",
    "\n",
    "training_set = dataset\n",
    "validation_set = TweebankDataset(\"/mnt/Others/experiments/datasets/Tweebank-dev/converted/en-ud-tweet-dev.fixed.conllu\")\n",
    "test_set = TweebankDataset(\"/mnt/Others/experiments/datasets/Tweebank-dev/converted/en-ud-tweet-test.fixed.conllu\")\n",
    "\n",
    "train_loader = DataLoader(training_set, shuffle=True, **dl_args)\n",
    "val_loader = DataLoader(validation_set, shuffle=False, **dl_args)\n",
    "test_loader = DataLoader(test_set, shuffle=False, **dl_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.583Z"
    }
   },
   "outputs": [],
   "source": [
    "assert training_set.tag_dict == validation_set.tag_dict == test_set.tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.584Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim: int, \n",
    "                 hidden_dim: int,  \n",
    "                 tagset_size: int,\n",
    "                 padding_idx=padding_idx, \n",
    "                 freeze_embeddings=True, \n",
    "                 w2v_weights=gnews_embeddings) -> None:\n",
    "        \n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.taget_size = tagset_size\n",
    "        \n",
    "        \n",
    "        embedding_tensors = torch.from_numpy(w2v_weights.vectors) # type: ignore        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(\n",
    "            embedding_tensors, freeze=freeze_embeddings, padding_idx=padding_idx)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "        \n",
    "        self.attention =  nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=0.1, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "        \n",
    "    def forward(self, words):\n",
    "        embeds = self.word_embeddings(words)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        \n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        relu_out = self.relu(attn_out)\n",
    "        \n",
    "        linear_out = self.linear(relu_out)\n",
    "\n",
    "        logits = F.log_softmax(linear_out, dim=-1)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.584Z"
    }
   },
   "outputs": [],
   "source": [
    "tagset_size = len(dataset.UNIQUE_TAGS)\n",
    "model = LSTMTagger(embedding_dim=300, hidden_dim=100,  tagset_size=tagset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out :: torch.Size([1, 18, 50])\n",
      "tags :: torch.Size([1, 50])\n",
      "tensor(2.8819)\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "# run a sample forward pass\n",
    "sample = dataset[42]\n",
    "\n",
    "with torch.no_grad():\n",
    "    words = sample[\"words\"].unsqueeze(0)\n",
    "    tags = sample[\"tags\"].unsqueeze(0).long()\n",
    "    \n",
    "    out = model(words)\n",
    "    \n",
    "    # apparently nllloss expects inputs in shape (bs, n_classes, feature_dims.......)\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss\n",
    "    out = rearrange(out, \"1 words probas -> 1 probas words\")\n",
    "    \n",
    "    print(f\"out :: {out.size()}\")\n",
    "    print(f\"tags :: {tags.size()}\")\n",
    "    \n",
    "# sample_loss = F.nll_loss(out, tags, ignore_index=17)\n",
    "sample_loss = nn.NLLLoss(ignore_index=17)\n",
    "print(sample_loss(input=out, target=tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 2, 1])\n",
      "tensor([[-2.5413,  0.5483,  0.8096],\n",
      "        [-0.0909, -0.7558,  0.7326],\n",
      "        [-1.3493,  1.3338, -1.0316],\n",
      "        [ 0.1246, -0.5146,  1.6468]])\n",
      "tensor(1.7221)\n"
     ]
    }
   ],
   "source": [
    "# https://discuss.pytorch.org/t/loss-function-for-multi-class-with-probabilities-as-output/60866\n",
    "\n",
    "x = torch.tensor([[0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 0]], dtype=torch.int32)\n",
    "_, i = torch.max(x, dim=-1)\n",
    "\n",
    "print(i)\n",
    "\n",
    "y = torch.randn((4, 3), dtype=torch.float32)\n",
    "print(y)\n",
    "\n",
    "print(F.nll_loss(F.log_softmax(y, dim=-1), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.586Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e135ebb14c7248cfb26cccdbfac6bd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:: [1]/[50] Step:: 0\n",
      "Train Loss:: 2.893364667892456 __________ Val Loss:: 2.8931798934936523\n",
      "Epoch:: [1]/[50] Step:: 10\n",
      "Train Loss:: 2.5961012840270996 __________ Val Loss:: 2.5972912311553955\n",
      "Epoch:: [2]/[50] Step:: 0\n",
      "Train Loss:: 2.5662412643432617 __________ Val Loss:: 2.5515384674072266\n",
      "Epoch:: [2]/[50] Step:: 10\n",
      "Train Loss:: 2.316964626312256 __________ Val Loss:: 2.3098161220550537\n",
      "Epoch:: [3]/[50] Step:: 0\n",
      "Train Loss:: 2.2772088050842285 __________ Val Loss:: 2.2694942951202393\n",
      "Epoch:: [3]/[50] Step:: 10\n",
      "Train Loss:: 2.209804058074951 __________ Val Loss:: 2.1998226642608643\n",
      "Epoch:: [4]/[50] Step:: 0\n",
      "Train Loss:: 2.2033324241638184 __________ Val Loss:: 2.1909267902374268\n",
      "Epoch:: [4]/[50] Step:: 10\n",
      "Train Loss:: 2.1314454078674316 __________ Val Loss:: 2.1405365467071533\n",
      "Epoch:: [5]/[50] Step:: 0\n",
      "Train Loss:: 2.1152122020721436 __________ Val Loss:: 2.116389751434326\n",
      "Epoch:: [5]/[50] Step:: 10\n",
      "Train Loss:: 1.9871525764465332 __________ Val Loss:: 1.9846967458724976\n",
      "Epoch:: [6]/[50] Step:: 0\n",
      "Train Loss:: 1.9431337118148804 __________ Val Loss:: 1.9573274850845337\n",
      "Epoch:: [6]/[50] Step:: 10\n",
      "Train Loss:: 1.904281735420227 __________ Val Loss:: 1.8161135911941528\n",
      "Epoch:: [7]/[50] Step:: 0\n",
      "Train Loss:: 1.7756898403167725 __________ Val Loss:: 1.7683011293411255\n",
      "Epoch:: [7]/[50] Step:: 10\n",
      "Train Loss:: 1.637947916984558 __________ Val Loss:: 1.65202796459198\n",
      "Epoch:: [8]/[50] Step:: 0\n",
      "Train Loss:: 1.642412781715393 __________ Val Loss:: 1.6156554222106934\n",
      "Epoch:: [8]/[50] Step:: 10\n",
      "Train Loss:: 1.5229899883270264 __________ Val Loss:: 1.529970645904541\n",
      "Epoch:: [9]/[50] Step:: 0\n",
      "Train Loss:: 1.4967857599258423 __________ Val Loss:: 1.4946907758712769\n",
      "Epoch:: [9]/[50] Step:: 10\n",
      "Train Loss:: 1.4196408987045288 __________ Val Loss:: 1.4115592241287231\n",
      "Epoch:: [10]/[50] Step:: 0\n",
      "Train Loss:: 1.3462625741958618 __________ Val Loss:: 1.391676902770996\n",
      "Epoch:: [10]/[50] Step:: 10\n",
      "Train Loss:: 1.3286551237106323 __________ Val Loss:: 1.3231966495513916\n",
      "Epoch:: [11]/[50] Step:: 0\n",
      "Train Loss:: 1.2985074520111084 __________ Val Loss:: 1.3173106908798218\n",
      "Epoch:: [11]/[50] Step:: 10\n",
      "Train Loss:: 1.2028584480285645 __________ Val Loss:: 1.2460429668426514\n",
      "Epoch:: [12]/[50] Step:: 0\n",
      "Train Loss:: 1.195641040802002 __________ Val Loss:: 1.2579973936080933\n",
      "Epoch:: [12]/[50] Step:: 10\n",
      "Train Loss:: 1.2225109338760376 __________ Val Loss:: 1.186165452003479\n",
      "Epoch:: [13]/[50] Step:: 0\n",
      "Train Loss:: 1.1317658424377441 __________ Val Loss:: 1.170904278755188\n",
      "Epoch:: [13]/[50] Step:: 10\n",
      "Train Loss:: 1.1052263975143433 __________ Val Loss:: 1.1213868856430054\n",
      "Epoch:: [14]/[50] Step:: 0\n",
      "Train Loss:: 1.0864927768707275 __________ Val Loss:: 1.1136361360549927\n",
      "Epoch:: [14]/[50] Step:: 10\n",
      "Train Loss:: 1.0386894941329956 __________ Val Loss:: 1.0741139650344849\n",
      "Epoch:: [15]/[50] Step:: 0\n",
      "Train Loss:: 1.0434999465942383 __________ Val Loss:: 1.0666316747665405\n",
      "Epoch:: [15]/[50] Step:: 10\n",
      "Train Loss:: 0.9736170172691345 __________ Val Loss:: 1.0422009229660034\n",
      "Epoch:: [16]/[50] Step:: 0\n",
      "Train Loss:: 0.9798482060432434 __________ Val Loss:: 1.0405769348144531\n",
      "Epoch:: [16]/[50] Step:: 10\n",
      "Train Loss:: 0.9199016690254211 __________ Val Loss:: 1.0277519226074219\n",
      "Epoch:: [17]/[50] Step:: 0\n",
      "Train Loss:: 1.0068565607070923 __________ Val Loss:: 1.0245431661605835\n",
      "Epoch:: [17]/[50] Step:: 10\n",
      "Train Loss:: 0.9279152750968933 __________ Val Loss:: 0.9866823554039001\n",
      "Epoch:: [18]/[50] Step:: 0\n",
      "Train Loss:: 0.8586300015449524 __________ Val Loss:: 0.9967067837715149\n",
      "Epoch:: [18]/[50] Step:: 10\n",
      "Train Loss:: 0.9036756157875061 __________ Val Loss:: 0.9528568387031555\n",
      "Epoch:: [19]/[50] Step:: 0\n",
      "Train Loss:: 0.8755127787590027 __________ Val Loss:: 0.947114884853363\n",
      "Epoch:: [19]/[50] Step:: 10\n",
      "Train Loss:: 0.8879767060279846 __________ Val Loss:: 0.9501690864562988\n",
      "Epoch:: [20]/[50] Step:: 0\n",
      "Train Loss:: 0.78178870677948 __________ Val Loss:: 0.9443865418434143\n",
      "Epoch:: [20]/[50] Step:: 10\n",
      "Train Loss:: 0.8529838919639587 __________ Val Loss:: 0.9203224182128906\n",
      "Epoch:: [21]/[50] Step:: 0\n",
      "Train Loss:: 0.8005218505859375 __________ Val Loss:: 0.9076372981071472\n",
      "Epoch:: [21]/[50] Step:: 10\n",
      "Train Loss:: 0.8321629166603088 __________ Val Loss:: 0.9031280875205994\n",
      "Epoch:: [22]/[50] Step:: 0\n",
      "Train Loss:: 0.7540349364280701 __________ Val Loss:: 0.9000424742698669\n",
      "Epoch:: [22]/[50] Step:: 10\n",
      "Train Loss:: 0.8131278157234192 __________ Val Loss:: 0.884939432144165\n",
      "Epoch:: [23]/[50] Step:: 0\n",
      "Train Loss:: 0.8130513429641724 __________ Val Loss:: 0.9030992388725281\n",
      "Epoch:: [23]/[50] Step:: 10\n",
      "Train Loss:: 0.7766168117523193 __________ Val Loss:: 0.8756646513938904\n",
      "Epoch:: [24]/[50] Step:: 0\n",
      "Train Loss:: 0.7137051224708557 __________ Val Loss:: 0.8812543749809265\n",
      "Epoch:: [24]/[50] Step:: 10\n",
      "Train Loss:: 0.7163233160972595 __________ Val Loss:: 0.8835194110870361\n",
      "Epoch:: [25]/[50] Step:: 0\n",
      "Train Loss:: 0.7246074080467224 __________ Val Loss:: 0.8720921874046326\n",
      "Epoch:: [25]/[50] Step:: 10\n",
      "Train Loss:: 0.6720384359359741 __________ Val Loss:: 0.8699538111686707\n",
      "Epoch:: [26]/[50] Step:: 0\n",
      "Train Loss:: 0.6418828368186951 __________ Val Loss:: 0.8724517822265625\n",
      "Epoch:: [26]/[50] Step:: 10\n",
      "Train Loss:: 0.72081458568573 __________ Val Loss:: 0.8684232234954834\n",
      "Epoch:: [27]/[50] Step:: 0\n",
      "Train Loss:: 0.6986450552940369 __________ Val Loss:: 0.8600913882255554\n",
      "Epoch:: [27]/[50] Step:: 10\n",
      "Train Loss:: 0.6286572813987732 __________ Val Loss:: 0.8509097695350647\n",
      "Epoch:: [28]/[50] Step:: 0\n",
      "Train Loss:: 0.6396445631980896 __________ Val Loss:: 0.867845356464386\n",
      "Epoch:: [28]/[50] Step:: 10\n",
      "Train Loss:: 0.6717523336410522 __________ Val Loss:: 0.8774703145027161\n",
      "Epoch:: [29]/[50] Step:: 0\n",
      "Train Loss:: 0.6419706344604492 __________ Val Loss:: 0.869884729385376\n",
      "Epoch:: [29]/[50] Step:: 10\n",
      "Train Loss:: 0.586905300617218 __________ Val Loss:: 0.8645099997520447\n",
      "Epoch:: [30]/[50] Step:: 0\n",
      "Train Loss:: 0.6011847257614136 __________ Val Loss:: 0.8598990440368652\n",
      "Epoch:: [30]/[50] Step:: 10\n",
      "Train Loss:: 0.6252027153968811 __________ Val Loss:: 0.8721786141395569\n",
      "Epoch:: [31]/[50] Step:: 0\n",
      "Train Loss:: 0.6473789811134338 __________ Val Loss:: 0.8716500401496887\n",
      "Epoch:: [31]/[50] Step:: 10\n",
      "Train Loss:: 0.5917866826057434 __________ Val Loss:: 0.8718409538269043\n",
      "Epoch:: [32]/[50] Step:: 0\n",
      "Train Loss:: 0.5771728754043579 __________ Val Loss:: 0.8598728179931641\n",
      "Epoch:: [32]/[50] Step:: 10\n",
      "Train Loss:: 0.6281156539916992 __________ Val Loss:: 0.8737001419067383\n",
      "Epoch:: [33]/[50] Step:: 0\n",
      "Train Loss:: 0.5702622532844543 __________ Val Loss:: 0.8784419894218445\n",
      "Epoch:: [33]/[50] Step:: 10\n",
      "Train Loss:: 0.5486912727355957 __________ Val Loss:: 0.9116649627685547\n",
      "Epoch:: [34]/[50] Step:: 0\n",
      "Train Loss:: 0.6148286461830139 __________ Val Loss:: 0.8846182823181152\n",
      "Epoch:: [34]/[50] Step:: 10\n",
      "Train Loss:: 0.5597550868988037 __________ Val Loss:: 0.8846678137779236\n",
      "Epoch:: [35]/[50] Step:: 0\n",
      "Train Loss:: 0.5267511010169983 __________ Val Loss:: 0.9120369553565979\n",
      "Epoch:: [35]/[50] Step:: 10\n",
      "Train Loss:: 0.5342874526977539 __________ Val Loss:: 0.8946898579597473\n",
      "Epoch:: [36]/[50] Step:: 0\n",
      "Train Loss:: 0.5114879012107849 __________ Val Loss:: 0.906029224395752\n",
      "Epoch:: [36]/[50] Step:: 10\n",
      "Train Loss:: 0.570374071598053 __________ Val Loss:: 0.9065510630607605\n",
      "Epoch:: [37]/[50] Step:: 0\n",
      "Train Loss:: 0.5352119207382202 __________ Val Loss:: 0.9113950133323669\n",
      "Epoch:: [37]/[50] Step:: 10\n",
      "Train Loss:: 0.5307072997093201 __________ Val Loss:: 0.9288015365600586\n",
      "Epoch:: [38]/[50] Step:: 0\n",
      "Train Loss:: 0.450420618057251 __________ Val Loss:: 0.9162015318870544\n",
      "Epoch:: [38]/[50] Step:: 10\n",
      "Train Loss:: 0.5547534823417664 __________ Val Loss:: 0.9619367122650146\n",
      "Epoch:: [39]/[50] Step:: 0\n",
      "Train Loss:: 0.5286278128623962 __________ Val Loss:: 0.9386076927185059\n",
      "Epoch:: [39]/[50] Step:: 10\n",
      "Train Loss:: 0.506061315536499 __________ Val Loss:: 0.9332106709480286\n",
      "Epoch:: [40]/[50] Step:: 0\n",
      "Train Loss:: 0.5142558813095093 __________ Val Loss:: 0.9219344258308411\n",
      "Epoch:: [40]/[50] Step:: 10\n",
      "Train Loss:: 0.48729726672172546 __________ Val Loss:: 0.9470803141593933\n",
      "Epoch:: [41]/[50] Step:: 0\n",
      "Train Loss:: 0.4569595754146576 __________ Val Loss:: 0.92795330286026\n",
      "Epoch:: [41]/[50] Step:: 10\n",
      "Train Loss:: 0.4123796224594116 __________ Val Loss:: 0.9660112261772156\n",
      "Epoch:: [42]/[50] Step:: 0\n",
      "Train Loss:: 0.4857996106147766 __________ Val Loss:: 0.9966545701026917\n",
      "Epoch:: [42]/[50] Step:: 10\n",
      "Train Loss:: 0.5176920294761658 __________ Val Loss:: 0.9841299057006836\n",
      "Epoch:: [43]/[50] Step:: 0\n",
      "Train Loss:: 0.44771260023117065 __________ Val Loss:: 0.9747568964958191\n",
      "Epoch:: [43]/[50] Step:: 10\n",
      "Train Loss:: 0.40182703733444214 __________ Val Loss:: 0.9991164207458496\n",
      "Epoch:: [44]/[50] Step:: 0\n",
      "Train Loss:: 0.4358339011669159 __________ Val Loss:: 0.9857385754585266\n",
      "Epoch:: [44]/[50] Step:: 10\n",
      "Train Loss:: 0.4334736764431 __________ Val Loss:: 1.0282078981399536\n",
      "Epoch:: [45]/[50] Step:: 0\n",
      "Train Loss:: 0.42213869094848633 __________ Val Loss:: 1.0121010541915894\n",
      "Epoch:: [45]/[50] Step:: 10\n",
      "Train Loss:: 0.4457685947418213 __________ Val Loss:: 1.0069135427474976\n",
      "Epoch:: [46]/[50] Step:: 0\n",
      "Train Loss:: 0.39479702711105347 __________ Val Loss:: 1.0234867334365845\n",
      "Epoch:: [46]/[50] Step:: 10\n",
      "Train Loss:: 0.41234636306762695 __________ Val Loss:: 1.0569936037063599\n",
      "Epoch:: [47]/[50] Step:: 0\n",
      "Train Loss:: 0.3989908695220947 __________ Val Loss:: 1.0605411529541016\n",
      "Epoch:: [47]/[50] Step:: 10\n",
      "Train Loss:: 0.42232218384742737 __________ Val Loss:: 1.0358299016952515\n",
      "Epoch:: [48]/[50] Step:: 0\n",
      "Train Loss:: 0.3829464316368103 __________ Val Loss:: 1.0397038459777832\n",
      "Epoch:: [48]/[50] Step:: 10\n",
      "Train Loss:: 0.39403295516967773 __________ Val Loss:: 1.0345107316970825\n",
      "Epoch:: [49]/[50] Step:: 0\n",
      "Train Loss:: 0.394186407327652 __________ Val Loss:: 1.073402762413025\n",
      "Epoch:: [49]/[50] Step:: 10\n",
      "Train Loss:: 0.44335654377937317 __________ Val Loss:: 1.0526617765426636\n",
      "Epoch:: [50]/[50] Step:: 0\n",
      "Train Loss:: 0.41792401671409607 __________ Val Loss:: 1.0898650884628296\n",
      "Epoch:: [50]/[50] Step:: 10\n",
      "Train Loss:: 0.41947200894355774 __________ Val Loss:: 1.0734730958938599\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(params=model.parameters())\n",
    "# ignore the index for PAD\n",
    "criterion = nn.NLLLoss(ignore_index=training_set.tag_dict.get(\"<PAD>\"))  # type: ignore        \n",
    "run_validation_every_n_step = 10\n",
    "\n",
    "\n",
    "# fp16\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs = 50\n",
    "for e in trange(epochs):\n",
    "\n",
    "    steps = 0\n",
    "    for batch in train_loader:\n",
    "        # switch to train mode\n",
    "        model.train()\n",
    "        \n",
    "        words = batch[\"words\"]\n",
    "        tags = batch[\"tags\"].long()\n",
    "        \n",
    "        # send data to device\n",
    "        words = words.to(device)\n",
    "        tags = tags.to(device)\n",
    "        \n",
    "        # zero out optimizer to accumulate new grads\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            logits = model(words)\n",
    "            logits = rearrange(logits, \"bs words probas -> bs probas words\")\n",
    "            \n",
    "            # loss\n",
    "            loss = criterion(logits, tags)\n",
    "        \n",
    "        \n",
    "        # ======== validation ==============\n",
    "        if steps % run_validation_every_n_step == 0:\n",
    "            val_losses = []\n",
    "            \n",
    "            # switch context\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    words = val_batch[\"words\"]\n",
    "                    tags = val_batch[\"tags\"].long()\n",
    "                    \n",
    "                    words = words.to(device)\n",
    "                    tags = tags.to(device)\n",
    "                    \n",
    "                    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                        logits = model(words)\n",
    "                        logits = rearrange(logits, \"bs words probas -> bs probas words\")\n",
    "                        val_loss = criterion(logits, tags)\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                    # preds = torch.max(logits, dim=-1).indices\n",
    "\n",
    "                # log\n",
    "                print(f\"Epoch:: [{e + 1}]/[{epochs}] Step:: {steps}\")\n",
    "                print(f\"Train Loss:: {loss} __________ Val Loss:: {torch.mean(torch.tensor(val_losses))}\")\n",
    "        \n",
    "        # switch context\n",
    "        model.train()\n",
    "        scaler.scale(loss).backward()  # type: ignore\n",
    "        # loss.backward()\n",
    "        scaler.step(optimizer)\n",
    "        # optimizer.step()\n",
    "        scaler.update()\n",
    "        steps += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.587Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"saved.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5383723a85d76e4e9cac0d136e01f6d8a177dc9dc4ee2b9a35edd51227ec1b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
