{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd73fe2b",
   "metadata": {},
   "source": [
    "# Hate speech in Bangla\n",
    "\n",
    "Retreived from : https://www.kaggle.com/naurosromim/bengali-hate-speech-dataset/version/1\n",
    "\n",
    "\n",
    "Original Paper: https://arxiv.org/abs/2012.09686"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32076a30",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0eabea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>hate</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>যত্তসব পাপন শালার ফাজলামী!!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>পাপন শালা রে রিমান্ডে নেওয়া দরকার</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>শালা লুচ্চা দেখতে পাঠার মত দেখা যায়</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব</td>\n",
       "      <td>1</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  hate category\n",
       "0                     যত্তসব পাপন শালার ফাজলামী!!!!!     1   sports\n",
       "1                  পাপন শালা রে রিমান্ডে নেওয়া দরকার     1   sports\n",
       "2  জিল্লুর রহমান স্যারের ছেলে এতো বড় জারজ হবে এটা...     1   sports\n",
       "3                শালা লুচ্চা দেখতে পাঠার মত দেখা যায়     1   sports\n",
       "4   তুই তো শালা গাজা খাইছচ।তুর মার হেডায় খেলবে সাকিব     1   sports"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"data\", \"bn_hate.csv\")\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a1c882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Meme, TikTok and others',\n",
       " 'celebrity',\n",
       " 'crime',\n",
       " 'entertainment',\n",
       " 'politics',\n",
       " 'religion',\n",
       " 'sports'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = set(df[\"category\"].values)\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f89440",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- The paper doesn't mention any specific tokenization or representation methods, other than saying they've used gensim to train word2vec embeddings and fasttext. \n",
    "\n",
    "- Tokenization on social media words is difficult, because people often use non standard words, with different forms and often with wrong spelling. So what works on a regular Bangla corpus may not work well here. \n",
    "\n",
    "- The paper also doesn't use contextualized embeddings. They focus on distributional embeddings such as word2vec. \n",
    "\n",
    "Let's try with BNLP Toolkit First. If it doesn't work we can go for BERT tokenizer which uses byte pair encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e2a570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\miniconda3\\envs\\experiments\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['আমি', 'বাংলায়', 'গান', 'গাই']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bnlp import BasicTokenizer\n",
    "\n",
    "basic_tokenizer = BasicTokenizer()\n",
    "\n",
    "# test with a bn sentence\n",
    "tokens = basic_tokenizer.tokenize(\"আমি বাংলায় গান গাই\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f01c23f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['যত্তসব', 'পাপন', 'শালার', 'ফাজলামী', '!', '!', '!', '!', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now on a sentence from the corpus\n",
    "sample_sentence = df[\"sentence\"].values[0]\n",
    "tokens = basic_tokenizer.tokenize(sample_sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['যত্তসব', 'পাপন', 'শালার', 'ফাজলামী']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    return [token for token in tokens if token not in punctuation]\n",
    "  \n",
    "remove_punctuation(tokens)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for cleaning all the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df[\"sentence\"].values\n",
    "labels = df[\"hate\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and remove punctuation\n",
    "\n",
    "def clean_sentences(sentences):\n",
    "    cleaned_sentences = []\n",
    "    for s in sentences:\n",
    "        tokens = basic_tokenizer.tokenize(s)\n",
    "        cleaned_sentences.append(remove_punctuation(tokens))\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_sentences(sentences)\n",
    "y = [int(i) for i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The issue with pretrained Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus contains a lot of words which are not used in the standard form of Bangla. As a result, most of them will throw a key not found related error during lookup in the Word2Vec vocabulary. Besides, BN Word2Vec embeddings were trained on a very general and formal corpora and it doesn't make sense to use it here for informal text. Embeddings should be used based on the contents of a corpus. \n",
    "\n",
    "So what to do? Training own embeddings seems a better option then to get empty vectors from a pretrained word2vec model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a word2vec embedding on this corpus\n",
    "\n",
    "Well ..... Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# embedding_dim is the size of a word vector\n",
    "# e.g. the word2vec output for a single word\n",
    "def train_w2v_model(clean_sentences, embedding_dim, window_size):\n",
    "  model = Word2Vec(clean_sentences, vector_size=embedding_dim, window=window_size, min_count=1, max_vocab_size=10e3)\n",
    "  return model\n",
    "\n",
    "w2v_model = train_w2v_model(X, embedding_dim=300, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('বেয়াদব', 0.9781259894371033),\n",
       " ('বেটা', 0.9769532680511475),\n",
       " ('খোর', 0.9754504561424255),\n",
       " ('কাফের', 0.9747885465621948),\n",
       " ('বাটপার', 0.9723208546638489),\n",
       " ('চুদ', 0.9712998270988464),\n",
       " ('তাহেরি', 0.9704238176345825),\n",
       " ('বন্ড', 0.9700490236282349),\n",
       " ('লুচ্চা', 0.9699358344078064),\n",
       " ('হলি', 0.9685174822807312)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"শয়তান\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2786"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = w2v_model.wv.index_to_key\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode inputs with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encode_tokens_with_embed_idx: 100%|██████████| 30000/30000 [00:00<00:00, 173412.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def encode_X_tokens_with_embed_idx(X, corpus_embedding=w2v_model):\n",
    "    encoded_X = list()\n",
    "    \n",
    "    for i in tqdm(range(len(X)), desc=\"encode_tokens_with_embed_idx\"):\n",
    "        idxs = []\n",
    "        tokens = X[i]\n",
    "\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                idx = corpus_embedding.wv.key_to_index[token]\n",
    "            except:\n",
    "                # if token isn't in the vocab\n",
    "                idx = 0\n",
    "\n",
    "            idxs.append(idx)\n",
    "\n",
    "        encoded_X.append(idxs)\n",
    "\n",
    "    return encoded_X\n",
    "\n",
    "\n",
    "encoded_X = encode_X_tokens_with_embed_idx(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2500, 67, 293, 0]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "Left with 0 for a maximum sequence length\n",
    "\n",
    "Now to find that sequence length ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max([len(x) for x in encoded_X])\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pad: 100%|██████████| 30000/30000 [00:00<00:00, 384593.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def pad_tokens(encoded_X, seq_len=max_seq_len):\n",
    "    padded = np.zeros(\n",
    "        (len(encoded_X), seq_len),\n",
    "        dtype=np.int32\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(len(encoded_X)), desc=\"pad\"):\n",
    "        tokens = encoded_X[i]\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "\n",
    "        padded[i, -len(tokens):] = np.array(tokens)\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "padded_X = pad_tokens(encoded_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0, 2500,   67,  293,    0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, erm, a lot of sparse values. Let's see if it works well. Otherwise we can always go back and find a better compression technqiue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch dataset and dataloader\n",
    "\n",
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BNHateSpeechDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        return torch.tensor(sentence), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train and test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    padded_X, y, random_state=42, train_size=0.8\n",
    ")\n",
    "\n",
    "\n",
    "# train and validation split\n",
    "# https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, train_size=0.8, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = BNHateSpeechDataset(x_train, y_train)\n",
    "testset = BNHateSpeechDataset(x_test, y_test)\n",
    "valset = BNHateSpeechDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "validation_loader = DataLoader(valset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### CNN\n",
    "This model is based on https://arxiv.org/abs/1408.5882  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class HateSpeechCNN(nn.Module):\n",
    "    def __init__(self, freeze_embeddings=True):\n",
    "        super(HateSpeechCNN, self).__init__()\n",
    "\n",
    "        # properties\n",
    "        self.kernel_sizes = [3, 4, 5]\n",
    "        self.num_filters = 100\n",
    "        self.embedding_dim = w2v_model.wv.vector_size\n",
    "        self.output_size = 1\n",
    "        self.vocab_size = len(w2v_model.wv.index_to_key)\n",
    "\n",
    "        # convert embeddings to tensors!\n",
    "        self.corpus_embedding = torch.from_numpy(w2v_model.wv.vectors)\n",
    "\n",
    "        # neural network\n",
    "\n",
    "        # embedding layer\n",
    "        # by default we're freezing embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            self.corpus_embedding, freeze=freeze_embeddings)\n",
    "\n",
    "        # conv layers\n",
    "        # 3 conv layers, since 3 kernel sizes\n",
    "        self.conv1d = nn.ModuleList([\n",
    "            nn.Conv2d(1, self.num_filters,\n",
    "                      (k, self.embedding_dim), padding=(k - 2, 0))\n",
    "\n",
    "            for k in self.kernel_sizes\n",
    "        ])\n",
    "\n",
    "        # final linear layer\n",
    "        self.linear = nn.Linear(len(self.kernel_sizes)\n",
    "                                * self.num_filters, self.output_size)\n",
    "\n",
    "        # dropout and sigmoid\n",
    "        # why sigmoid? Well, binary classification task!\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # helper\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        \"\"\"\n",
    "        Convolutional + max pooling layer\n",
    "        \"\"\"\n",
    "        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "\n",
    "        # 1D pool over conv_seq_length\n",
    "        # squeeze to get size: (batch_size, num_filters)\n",
    "        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.unsqueeze(1)  # reshape for conv (vector to matrix)\n",
    "\n",
    "        conv_out = [self.conv_and_pool(embeds, conv) for conv in self.conv1d]\n",
    "\n",
    "        # concate convolution outputs as a \"vector\"\n",
    "        out = torch.cat(conv_out, 1)\n",
    "        # apply dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # linear\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HateSpeechCNN(\n",
      "  (embedding): Embedding(2786, 300)\n",
      "  (conv1d): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1), padding=(2, 0))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1), padding=(3, 0))\n",
      "  )\n",
      "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "cnn = HateSpeechCNN()\n",
    "print(cnn)\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 0.001\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cnn = cnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 297/600 [00:07<00:07, 42.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300\n",
      "Training loss: 0.5482604503631592\n",
      "========= Running Validation ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:03<00:00, 46.76it/s]\n",
      " 51%|█████     | 307/600 [00:10<00:46,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation loss: 0.41963126649459204\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 597/600 [00:17<00:00, 42.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 600\n",
      "Training loss: 0.21656543016433716\n",
      "========= Running Validation ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:03<00:00, 46.32it/s]\n",
      "100%|██████████| 600/600 [00:20<00:00, 29.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation loss: 0.42718827744325\n",
      "===================\n",
      "\n",
      "Epoch: 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 295/600 [00:06<00:07, 43.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 900\n",
      "Training loss: 0.332777202129364\n",
      "========= Running Validation ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:03<00:00, 46.41it/s]\n",
      " 51%|█████     | 306/600 [00:10<00:44,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation loss: 0.42447998374700546\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 596/600 [00:17<00:00, 42.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1200\n",
      "Training loss: 0.47631603479385376\n",
      "========= Running Validation ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:03<00:00, 46.08it/s]\n",
      "100%|██████████| 600/600 [00:20<00:00, 29.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation loss: 0.41533697923024493\n",
      "===================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "def train_cnn(model, train_loader, validation_loader, epochs, optimizer, loss_fn, device):\n",
    "    step_counter = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "      print(f\"Epoch: {e + 1}/{epochs}\")\n",
    "      # set to train mode\n",
    "      model.train()\n",
    "      for sample in tqdm(train_loader):\n",
    "        sentence, label = sample\n",
    "\n",
    "        # send to device\n",
    "        sentence = sentence.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        # zero gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        output = model(sentence)\n",
    "        loss = loss_fn(output.squeeze(), label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        step_counter += 1\n",
    "        \n",
    "        # log every 300 steps\n",
    "        if step_counter % 300 == 0:\n",
    "          # log loss\n",
    "          print(f\"Step: {step_counter}\")\n",
    "          print(f\"Training loss: {loss.item()}\")\n",
    "          print(\"========= Running Validation ==========\")\n",
    "\n",
    "          # validation\n",
    "          validation_losses = []\n",
    "          # set to eval mode\n",
    "          model.eval()\n",
    "          with torch.no_grad():\n",
    "            # run the forward pass on val loader\n",
    "            for sample in tqdm(validation_loader):\n",
    "              sentence, label = sample\n",
    "              sentence = sentence.to(device)\n",
    "              label = label.to(device)\n",
    "              val_output = model(sentence)\n",
    "\n",
    "              val_loss = loss_fn(val_output.squeeze(), label.float())\n",
    "              validation_losses.append(val_loss.item())\n",
    "\n",
    "            # print average validation loss\n",
    "            print(f\"Average Validation loss: {np.mean(validation_losses)}\")\n",
    "\n",
    "          print(\"===================\")\n",
    "          print()\n",
    "    \n",
    "\n",
    "# call train function\n",
    "train_cnn(\n",
    "  model=cnn, \n",
    "  train_loader=train_loader, \n",
    "  validation_loader=validation_loader, \n",
    "  epochs=2, \n",
    "  optimizer=optimizer, \n",
    "  loss_fn=loss_fn,\n",
    "  device=device)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1fc978d16eb19028606c17cdea149455d56c3edc1ed639336e4a95eee285175"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('experiments': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
