{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# dataset \n",
    "data_root = os.path.join(\n",
    "    os.getenv(\"EXPSTORE\"),\n",
    "    \"datasets\",\n",
    "    \"Tweebank-dev\",\n",
    "    \"converted\"\n",
    ")\n",
    "\n",
    "assert os.path.exists(data_root) == True\n",
    "\n",
    "data_splits = {\n",
    "    \"train\": \"en-ud-tweet-train.fixed.conllu\",\n",
    "    \"val\": \"en-ud-tweet-dev.fixed.conllu\",\n",
    "    \"test\": \"en-ud-tweet-test.fixed.conllu\"\n",
    "}\n",
    "\n",
    "\n",
    "class ConlluRowInfo:\n",
    "    word: str\n",
    "    lemma: str\n",
    "    pos: str\n",
    "\n",
    "    def __init__(self, word: str, lemma: str, pos: str) -> None:\n",
    "        self.word = word\n",
    "        self.lemma = lemma\n",
    "        self.pos = pos\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        rep = {\n",
    "            \"word\": self.word,\n",
    "            \"lemma\": self.lemma,\n",
    "            \"pos\": self.pos\n",
    "        }\n",
    "        return str(rep)\n",
    "\n",
    "\n",
    "class ConlluRow:\n",
    "    info: List[ConlluRowInfo]\n",
    "    # text: str\n",
    "\n",
    "    def __init__(self, infos: List[ConlluRowInfo]) -> None:\n",
    "        self.info = infos\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"info : {self.info}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    # ============ read ==============\n",
    "    with open(filename, \"r\") as f:\n",
    "        raw_data = f.readlines()\n",
    "        \n",
    "    # =============== process =============\n",
    "    lines = list()\n",
    "    buffer = list()\n",
    "    for _, line in tqdm(enumerate(raw_data), desc=\"reading lines from file\"):\n",
    "        if line == \"\\n\":\n",
    "            lines.append(buffer)\n",
    "            buffer = list()\n",
    "        else:\n",
    "            buffer.append(line)\n",
    "            \n",
    "        \n",
    "    # make sure that buffer is always empty after the loop ends\n",
    "    assert len(buffer) == 0\n",
    "    \n",
    "    # ========== orga in objects ============\n",
    "    processed_lines = list()\n",
    "    for idx, l in tqdm(enumerate(lines), desc=\"organising in objects\"):\n",
    "        l_info = list()\n",
    "        for info in l[2:]:\n",
    "            temp = info.split(\"\\t\")\n",
    "            \n",
    "            # need idx 1, 2,3 : word, lemma and pos\n",
    "            word = temp[1]\n",
    "            lemma = temp[2]\n",
    "            tag = temp[3]\n",
    "            \n",
    "            l_info.append(ConlluRowInfo(word, lemma, tag))\n",
    "            \n",
    "        processed_lines.append(ConlluRow(l_info))\n",
    "        \n",
    "    # ===========================================\n",
    "    return processed_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 128) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        positions = torch.arange(0, max_len)\n",
    "        # vectorize\n",
    "        positions = rearrange(positions, \"n -> n 1\")\n",
    "        \n",
    "        # but in log space\n",
    "        # -2i * n / d\n",
    "        # even steps , since 2i\n",
    "        # n = 10e3\n",
    "        denominator = -torch.arange(0, d_model, 2) * torch.log(torch.tensor(10.0e3) / d_model)\n",
    "        # exp since we took log from the original equation, which was 1/n^(2i / d)\n",
    "        denominator = torch.exp(denominator)\n",
    "        \n",
    "        # positional encoding tensor\n",
    "        pe = torch.zeros(size=(max_len, 1, d_model))\n",
    "        \n",
    "        # encode the first dim\n",
    "        pe[:, 0, 0::2] = torch.sin(positions * denominator)\n",
    "        # second dim\n",
    "        pe[:, 0, 1::2] = torch.cos(positions * denominator)\n",
    "        \n",
    "        # register as a buffer, variable but without gradient update\n",
    "        self.register_buffer(\"positional_encoding\", pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x has a shape of (seq_len, batch_size, embedding_dim)\n",
    "        # so you pass the embedded vectors for a sequence\n",
    "        \n",
    "        # residual connection + dropout\n",
    "        x = x + self.positional_encoding[:x.size(0)]  # type: ignore\n",
    "        return self.dropout(x)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosTaggerTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 d_model: int, \n",
    "                 n_heads: int, \n",
    "                 n_encoder_layers: int, \n",
    "                 n_decoder_layers: int, \n",
    "                 dropout: float,\n",
    "                 max_len: int, \n",
    "                 n_tags: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.positional_encoder = PositionalEncoding(d_model=self.d_model, max_len=max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=n_heads,\n",
    "            num_decoder_layers=n_encoder_layers,\n",
    "            num_encoder_layers=n_decoder_layers, \n",
    "            dropout=dropout,\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, n_tags)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        # ================ get embed ================\n",
    "        src = self.embedding(source) * torch.sqrt(torch.tensor(self.d_model))\n",
    "        tgt = self.embedding(target) * torch.sqrt(torch.tensor(self.d_model))\n",
    "        \n",
    "        # ================ rearrange the shapes =================\n",
    "        # default shape : bs, seq, embed\n",
    "        # transformer needs : seq, bs, embed\n",
    "        # can also use permute but this approach is more intuitive\n",
    "        src = rearrange(src, \"bs seq embed -> seq bs embed\")\n",
    "        tgt = rearrange(tgt, \"bs seq embed -> seq bs embed\")\n",
    "        \n",
    "        # =================== pos enc ================\n",
    "        src_pe = self.positional_encoder(src)\n",
    "        tgt_pe = self.positional_encoder(tgt)\n",
    "        \n",
    "        # =========== pass through transformer =============\n",
    "        out = self.transformer(src_pe, tgt_pe)\n",
    "        \n",
    "        # ================ final linear layer ================\n",
    "        out = self.linear(out)\n",
    "        out = self.log_softmax(out)\n",
    "        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TweebankDataset(Dataset):\n",
    "    def __init__(self, file_name: str, max_seq_len: int, file_reader_fn=read_data) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.MAX_SEQ_LEN = max_seq_len\n",
    "        \n",
    "        # ================ tags ===================\n",
    "        \n",
    "        self.UNIQUE_TAGS = ['PRON', 'NUM', 'NOUN', 'CCONJ', 'ADV', 'SCONJ', \n",
    "                               'ADP', 'AUX', 'PROPN', 'SYM', 'DET', \n",
    "                               'INTJ', 'PUNCT', 'X', 'ADJ', 'VERB', 'PART', '</PAD>']\n",
    "        self.tag_dict = dict()\n",
    "        self.__encode_tags()\n",
    "        \n",
    "        self.n_classes = len(self.UNIQUE_TAGS)\n",
    "        \n",
    "        # ================= data ===================\n",
    "        self.data = file_reader_fn(file_name)\n",
    "        \n",
    "        # ============== vocab =====================\n",
    "        self.vocab = list()\n",
    "        self.__build_vocab()\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        self.word_dict = dict()\n",
    "        self.__encode_words()\n",
    "        \n",
    "    # ======================= tag encoding ===============\n",
    "    def __encode_tags(self) -> None:\n",
    "        for idx, tag in enumerate(self.UNIQUE_TAGS):\n",
    "            self.tag_dict[tag] = idx\n",
    "            \n",
    "    # ======================= vocab building and encoding ===============\n",
    "    def __build_vocab(self) -> None:\n",
    "        vocabulary = set()\n",
    "        for idx in range(len(self.data)):\n",
    "            words = [i.word for i in self.data[idx].info]\n",
    "            for w in words:\n",
    "                vocabulary.add(w)\n",
    "        \n",
    "        # ============ add oov and pad ===============\n",
    "        vocabulary.add(\"</OOV>\")\n",
    "        vocabulary.add(\"</PAD>\")\n",
    "        self.vocab = list(vocabulary)\n",
    "        \n",
    "    def __encode_words(self) -> None:\n",
    "        for idx, word in enumerate(self.vocab):\n",
    "            self.word_dict[word] = idx\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # ========================== dataset methods =================   \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = [i.word for i in self.data[idx].info]\n",
    "        tags = [i.pos for i in self.data[idx].info]\n",
    "        \n",
    "        # ============ convert to ids =================\n",
    "        # using idx from vocab\n",
    "        oov_idx = self.word_dict[\"</OOV>\"]\n",
    "        word_ids = list()\n",
    "        \n",
    "        for w in words:\n",
    "            if w in self.word_dict.keys():\n",
    "                word_ids.append(self.word_dict[w])\n",
    "            else:\n",
    "                word_ids.append(oov_idx)\n",
    "                \n",
    "        # ================ same for tags ============\n",
    "        tag_ids = list()\n",
    "        for t in tags:\n",
    "            tag_ids.append(self.tag_dict[t])\n",
    "        \n",
    "        # ============== pad words ===============\n",
    "        padding_idx = self.word_dict[\"</PAD>\"]\n",
    "\n",
    "        \n",
    "        # left pad\n",
    "        padded_words = torch.ones(self.MAX_SEQ_LEN, dtype=torch.long) * padding_idx\n",
    "        padded_words[-len(word_ids):] = torch.tensor(word_ids)\n",
    "        \n",
    "        padded_tags = torch.ones(self.MAX_SEQ_LEN, dtype=torch.long) * self.tag_dict.get(\"</PAD>\")  # type: ignore        \n",
    "        padded_tags[-len(tags):] = torch.tensor(tag_ids)\n",
    "        \n",
    "        return {\n",
    "            \"source\": padded_words, \n",
    "            \"targets\": padded_tags\n",
    "        }   \n",
    "        \n",
    "\n",
    "# ds = TweebankDataset(os.path.join(data_root, data_splits[\"train\"]), 128)\n",
    "# for d in ds:\n",
    "#     print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cdcbc99f514b938d64015230f6bc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading lines from file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39abfdc9bdb141b8b11d5e39c187d92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "organising in objects: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0c5c33ad1348e9904a279b8e2fd213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading lines from file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e6dfcf0ebd410da3a307177b014baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "organising in objects: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4b72e2ea1043858e6fba6ffad2f517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading lines from file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dff624caf443d39c14235dbf93b234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "organising in objects: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# device\n",
    "device = \"cuda\"\n",
    "\n",
    "# hparams\n",
    "max_len = 128\n",
    "d_model = 512\n",
    "\n",
    "\n",
    "trainset = TweebankDataset(os.path.join(data_root, data_splits[\"train\"]), max_len)\n",
    "valset = TweebankDataset(os.path.join(data_root, data_splits[\"val\"]), max_len)\n",
    "testset = TweebankDataset(os.path.join(data_root, data_splits[\"test\"]), max_len)\n",
    "\n",
    "train_vocab_size = trainset.vocab_size\n",
    "\n",
    "\n",
    "data_loader_args = {\n",
    "    \"pin_memory\": True,\n",
    "    \"batch_size\": 64,\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(trainset, shuffle=True, **data_loader_args)\n",
    "val_loader = DataLoader(valset, shuffle=False, **data_loader_args)\n",
    "test_loader = DataLoader(testset, shuffle=False, **data_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "model = PosTaggerTransformer(\n",
    "    vocab_size=train_vocab_size,\n",
    "    d_model=d_model, \n",
    "    n_heads=4, \n",
    "    n_encoder_layers=4, \n",
    "    n_decoder_layers=4, \n",
    "    dropout=0.1, \n",
    "    max_len=max_len, \n",
    "    n_tags=trainset.n_classes)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.NLLLoss(ignore_index=trainset.tag_dict[\"</PAD>\"])\n",
    "\n",
    "# fp16\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs = 5\n",
    "run_validation_every_n_step = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284eb27a056449588d88b6b88824704e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:: [1]/[5] Step:: 0\n",
      "Train Loss:: 2.969268560409546 __________ Val Loss:: 2.973590850830078\n",
      "Epoch:: [1]/[5] Step:: 10\n",
      "Train Loss:: 2.579211711883545 __________ Val Loss:: 2.5029423236846924\n",
      "Epoch:: [1]/[5] Step:: 20\n",
      "Train Loss:: 1.1059503555297852 __________ Val Loss:: 0.9716982245445251\n",
      "Epoch:: [2]/[5] Step:: 0\n",
      "Train Loss:: 0.9828959703445435 __________ Val Loss:: 0.9093950390815735\n",
      "Epoch:: [2]/[5] Step:: 10\n",
      "Train Loss:: 0.13950476050376892 __________ Val Loss:: 0.10858168452978134\n",
      "Epoch:: [2]/[5] Step:: 20\n",
      "Train Loss:: 0.02538725547492504 __________ Val Loss:: 0.014244730584323406\n",
      "Epoch:: [3]/[5] Step:: 0\n",
      "Train Loss:: 0.007921725511550903 __________ Val Loss:: 0.0035086784046143293\n",
      "Epoch:: [3]/[5] Step:: 10\n",
      "Train Loss:: 0.002956085605546832 __________ Val Loss:: 0.001426293863914907\n",
      "Epoch:: [3]/[5] Step:: 20\n",
      "Train Loss:: 0.0016054840525612235 __________ Val Loss:: 0.0007603598642162979\n",
      "Epoch:: [4]/[5] Step:: 0\n",
      "Train Loss:: 0.0020909642335027456 __________ Val Loss:: 0.0005968895275145769\n",
      "Epoch:: [4]/[5] Step:: 10\n",
      "Train Loss:: 0.0011500169057399035 __________ Val Loss:: 0.00047232446377165616\n",
      "Epoch:: [4]/[5] Step:: 20\n",
      "Train Loss:: 0.0008463392150588334 __________ Val Loss:: 0.00038169222534634173\n",
      "Epoch:: [5]/[5] Step:: 0\n",
      "Train Loss:: 0.0007883208454586565 __________ Val Loss:: 0.00035251304507255554\n",
      "Epoch:: [5]/[5] Step:: 10\n",
      "Train Loss:: 0.0006885132170282304 __________ Val Loss:: 0.00032035019830800593\n",
      "Epoch:: [5]/[5] Step:: 20\n",
      "Train Loss:: 0.0006775556830689311 __________ Val Loss:: 0.0002897718222811818\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange\n",
    "\n",
    "for e in trange(epochs):\n",
    "\n",
    "    steps = 0\n",
    "    for batch in train_loader:\n",
    "        # switch to train mode\n",
    "        model.train()\n",
    "\n",
    "        source = batch[\"source\"]\n",
    "        targets = batch[\"targets\"].long()\n",
    "\n",
    "        # send data to device\n",
    "        source = source.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # zero out optimizer to accumulate new grads\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            logits = model(source, targets)\n",
    "            logits = rearrange(logits, \"seq bs probas -> bs probas seq\")\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "        # ======== validation ==============\n",
    "        if steps % run_validation_every_n_step == 0:\n",
    "            val_losses = []\n",
    "\n",
    "            # switch context\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    source = val_batch[\"source\"]\n",
    "                    targets = val_batch[\"targets\"].long()\n",
    "\n",
    "                    source = source.to(device)\n",
    "                    targets = targets.to(device)\n",
    "\n",
    "                    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                        logits = model(source, targets)\n",
    "                        logits = rearrange(\n",
    "                            logits, \"seq bs probas -> bs probas seq\")\n",
    "                        val_loss = criterion(logits, targets)\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                    # preds = torch.max(logits, dim=-1).indices\n",
    "\n",
    "                # log\n",
    "                print(f\"Epoch:: [{e + 1}]/[{epochs}] Step:: {steps}\")\n",
    "                print(\n",
    "                    f\"Train Loss:: {loss} __________ Val Loss:: {torch.mean(torch.tensor(val_losses))}\")\n",
    "\n",
    "        # switch context\n",
    "        model.train()\n",
    "        scaler.scale(loss).backward()  # type: ignore\n",
    "        # loss.backward()\n",
    "        scaler.step(optimizer)\n",
    "        # optimizer.step()\n",
    "        scaler.update()\n",
    "        steps += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"saved.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5383723a85d76e4e9cac0d136e01f6d8a177dc9dc4ee2b9a35edd51227ec1b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
