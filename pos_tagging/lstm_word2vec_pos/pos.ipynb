{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-25T21:39:59.080877Z",
     "start_time": "2022-12-25T21:39:55.576076Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /home/shawon/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as dl\n",
    "\n",
    "pretrained_weights_name = \"word2vec-google-news-300\"\n",
    "model_dl_path = os.path.join(\n",
    "    dl.BASE_DIR, pretrained_weights_name, f\"{pretrained_weights_name}.gz\")\n",
    "\n",
    "\n",
    "if os.path.exists(model_dl_path):\n",
    "    # load model\n",
    "    print(f\"Loading model from {model_dl_path}\")\n",
    "    gnews_embeddings = dl.load(pretrained_weights_name)\n",
    "else:\n",
    "    # download\n",
    "    print(f\"Model will be downloaded at {model_dl_path}\")\n",
    "    gnews_embeddings = dl.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.575Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawon/anaconda3/envs/exp/lib/python3.10/site-packages/gensim/models/keyedvectors.py:552: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# add PAD to embeddings\n",
    "\n",
    "# 0 padding, 300 embedding dims\n",
    "gnews_embeddings.add_vector('</PAD>', np.zeros(300)) # type: ignore\n",
    "gnews_embeddings.add_vector('</OOV>', np.ones(300) * -1)  # type: ignore\n",
    "\n",
    "# need it later for loading the embeddings in pytorch model\n",
    "padding_idx = gnews_embeddings.get_index(\"</PAD>\") # type: ignore\n",
    "oov_idx = gnews_embeddings.get_index(\"</OOV>\") # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000\n",
      "3000001\n"
     ]
    }
   ],
   "source": [
    "print(padding_idx)\n",
    "print(oov_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.576Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/Oneplus/Tweebank\n",
    "\n",
    "train_file = os.path.join(\n",
    "    \"/mnt/Others/experiments/datasets/Tweebank-dev/converted/\"\n",
    "    \"en-ud-tweet-train.fixed.conllu\")\n",
    "\n",
    "# assert os.path.exists(train_file)\n",
    "\n",
    "with open(train_file) as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# tweet_id = feb_jul_16.1463316480\\n',\n",
       " \"# text = RT @USER991: Dear diary,       I've been rapping in 3 accents and no longer know which one is truly mine. I am a sadting - Drake URL217…\\n\",\n",
       " '1\\tRT\\trt\\tX\\t_\\t_\\t10\\tdiscourse\\t_\\t_\\n',\n",
       " '2\\t@USER991\\t@USER\\tX\\t_\\t_\\t1\\tdiscourse\\t_\\tSpaceAfter=No\\n',\n",
       " '3\\t:\\t:\\tPUNCT\\t_\\t_\\t1\\tpunct\\t_\\t_\\n',\n",
       " '4\\tDear\\tdear\\tADJ\\t_\\t_\\t5\\tamod\\t_\\t_\\n',\n",
       " '5\\tdiary\\tdiary\\tNOUN\\t_\\t_\\t10\\tvocative\\t_\\tSpaceAfter=No\\n',\n",
       " '6\\t,\\t,\\tPUNCT\\t_\\t_\\t10\\tpunct\\t_\\t_\\n',\n",
       " '7\\tI\\ti\\tPRON\\t_\\t_\\t10\\tnsubj\\t_\\tSpaceAfter=No\\n',\n",
       " \"8\\t've\\t've\\tAUX\\t_\\t_\\t10\\taux\\t_\\t_\\n\",\n",
       " '9\\tbeen\\tbe\\tAUX\\t_\\t_\\t10\\taux\\t_\\t_\\n',\n",
       " '10\\trapping\\trap\\tVERB\\t_\\t_\\t0\\troot\\t_\\t_\\n',\n",
       " '11\\tin\\tin\\tADP\\t_\\t_\\t13\\tcase\\t_\\t_\\n',\n",
       " '12\\t3\\tNUMBER\\tNUM\\t_\\t_\\t13\\tnummod\\t_\\t_\\n',\n",
       " '13\\taccents\\taccent\\tNOUN\\t_\\t_\\t10\\tobl\\t_\\t_\\n',\n",
       " '14\\tand\\tand\\tCCONJ\\t_\\t_\\t17\\tcc\\t_\\t_\\n',\n",
       " '15\\tno\\tno\\tADV\\t_\\t_\\t16\\tadvmod\\t_\\t_\\n',\n",
       " '16\\tlonger\\tlonger\\tADV\\t_\\t_\\t17\\tadvmod\\t_\\t_\\n',\n",
       " '17\\tknow\\tknow\\tVERB\\t_\\t_\\t10\\tconj\\t_\\t_\\n',\n",
       " '18\\twhich\\twhich\\tDET\\t_\\t_\\t19\\tdet\\t_\\t_\\n',\n",
       " '19\\tone\\tone\\tNUM\\t_\\t_\\t22\\tnsubj\\t_\\t_\\n',\n",
       " '20\\tis\\tbe\\tAUX\\t_\\t_\\t22\\tcop\\t_\\t_\\n',\n",
       " '21\\ttruly\\ttruly\\tADV\\t_\\t_\\t22\\tadvmod\\t_\\t_\\n',\n",
       " '22\\tmine\\tmine\\tPRON\\t_\\t_\\t17\\tccomp\\t_\\tSpaceAfter=No\\n',\n",
       " '23\\t.\\t.\\tPUNCT\\t_\\t_\\t10\\tpunct\\t_\\t_\\n',\n",
       " '24\\tI\\ti\\tPRON\\t_\\t_\\t27\\tnsubj\\t_\\t_\\n',\n",
       " '25\\tam\\tbe\\tAUX\\t_\\t_\\t27\\tcop\\t_\\t_\\n',\n",
       " '26\\ta\\ta\\tDET\\t_\\t_\\t27\\tdet\\t_\\t_\\n',\n",
       " '27\\tsadting\\tsadting\\tNOUN\\t_\\t_\\t10\\tparataxis\\t_\\t_\\n',\n",
       " '28\\t-\\t-\\tPUNCT\\t_\\t_\\t27\\tpunct\\t_\\t_\\n',\n",
       " '29\\tDrake\\tdrake\\tPROPN\\t_\\t_\\t27\\tparataxis\\t_\\t_\\n',\n",
       " '30\\tURL217\\tURL\\tX\\t_\\t_\\t27\\tlist\\t_\\tSpaceAfter=No\\n',\n",
       " '31\\t…\\t…\\tPUNCT\\t_\\t_\\t27\\tpunct\\t_\\tSpaceAfter=\\\\n\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break line at every \"\\n\"\n",
    "tweets = list()\n",
    "buffer = list()\n",
    "for idx, tw in enumerate(data):\n",
    "    if tw == \"\\n\":\n",
    "        # one partition here\n",
    "        tweets.append(buffer)\n",
    "        buffer = []\n",
    "    else:\n",
    "        # keep appending\n",
    "        buffer.append(tw)\n",
    "        \n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', 'Dear', 'dear', 'ADJ', '_', '_', '5', 'amod', '_', '_\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format for tokens\n",
    "# number - word - lemma - pos - _ - _ - id - role, -, - \n",
    "\n",
    "'4\\tDear\\tdear\\tADJ\\t_\\t_\\t5\\tamod\\t_\\t_\\n'.split(\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.578Z"
    }
   },
   "outputs": [],
   "source": [
    "# need idx 1, 2,3 : word, lemma and pos\n",
    "\n",
    "class ConlluRowInfo:\n",
    "    word: str\n",
    "    lemma: str\n",
    "    pos: str\n",
    "    \n",
    "    def __init__(self, word: str, lemma: str, pos: str) -> None:\n",
    "        self.word = word\n",
    "        self.lemma = lemma\n",
    "        self.pos = pos\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        rep = {\n",
    "            \"word\": self.word,\n",
    "            \"lemma\": self.lemma,\n",
    "            \"pos\": self.pos\n",
    "        }\n",
    "        return str(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.579Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class ConlluRow:\n",
    "    info: List[ConlluRowInfo]\n",
    "    # text: str\n",
    "    \n",
    "    def __init__(self, infos: List[ConlluRowInfo]) -> None:\n",
    "        self.info = infos\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        return f\"info : {self.info}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.579Z"
    }
   },
   "outputs": [],
   "source": [
    "structured_tweets = list()\n",
    "\n",
    "for tweet in tweets:\n",
    "    # text = tweet[1].replace(\"# text = \", \"\")\n",
    "    info_in_tweet = list()\n",
    "    # start from idx 2\n",
    "    for infos in tweet[2:]:\n",
    "        buffer = infos.split(\"\\t\")\n",
    "        try:\n",
    "            word = buffer[1]\n",
    "            lemma = buffer[2]\n",
    "            tag = buffer[3]\n",
    "            info_in_tweet.append(ConlluRowInfo(word, lemma, tag))\n",
    "        except IndexError:\n",
    "            print(buffer)\n",
    "        except AttributeError as e:\n",
    "            print(e.name)\n",
    "    structured_tweets.append(ConlluRow(info_in_tweet))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.8046875"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(structured_tweets) / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.580Z"
    }
   },
   "outputs": [],
   "source": [
    "# time to define the torch dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import trange, tqdm\n",
    "from typing import Dict, List\n",
    "\n",
    "class TweebankDataset(Dataset):\n",
    "    def __init__(self, filename, oov_idx=oov_idx, padding_idx=padding_idx, w2v_weights=gnews_embeddings) -> None:\n",
    "        self.filename = filename\n",
    "        \n",
    "        self.w2v = w2v_weights\n",
    "        self.oov_idx = oov_idx\n",
    "        self.padding_idx = padding_idx\n",
    "        self.data = list()\n",
    "        self.__read_data()\n",
    "        \n",
    "        self.MAX_SEQ_LEN = 50 # default value\n",
    "        # self.__find_max_seq_len()\n",
    "        \n",
    "        self.UNIQUE_TAGS = ['PRON', 'NUM', 'NOUN', 'CCONJ', 'ADV', 'SCONJ', \n",
    "                               'ADP', 'AUX', 'PROPN', 'SYM', 'DET', \n",
    "                               'INTJ', 'PUNCT', 'X', 'ADJ', 'VERB', 'PART', '</PAD>']\n",
    "        self.tag_dict = dict()\n",
    "        self.__encode_tags()\n",
    "        \n",
    "        self.number_tags = len(self.UNIQUE_TAGS)\n",
    "        \n",
    "        self.vocabulary = self.w2v.index_to_key  # type: ignore\n",
    "            \n",
    "    \n",
    "    def __len__(self) ->  int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        # ============== collect ===================\n",
    "        words = [i.word for i in self.data[idx].info]\n",
    "        # lemmas = [i.lemma for i in self.data[idx].info]\n",
    "        tags = [i.pos for i in self.data[idx].info]\n",
    "        \n",
    "        \n",
    "        # =================== convert using word2vec weights ==========\n",
    "        for idx in range(len(words)):\n",
    "            try:\n",
    "                w2v_idx = self.w2v.key_to_index[words[idx]]  # type: ignore \n",
    "            except KeyError:\n",
    "                # indicates OOV\n",
    "                w2v_idx = self.oov_idx\n",
    "            words[idx] = w2v_idx\n",
    "            tags[idx] = self.tag_dict[tags[idx]]\n",
    "            \n",
    "        \n",
    "        # ============== pad words ===============\n",
    "        # left pad\n",
    "        padded_words = np.ones(self.MAX_SEQ_LEN, dtype=np.int32) * self.padding_idx\n",
    "        padded_words[-len(words):] = words\n",
    "        \n",
    "        # ============== pad tags =================\n",
    "        padded_tags = np.ones(self.MAX_SEQ_LEN, dtype=np.int32) * self.tag_dict.get(\"</PAD>\")  # type: ignore        \n",
    "        padded_tags[-len(tags):] = tags\n",
    "        \n",
    "        return {\n",
    "            \"words\": torch.tensor(padded_words),\n",
    "            \"tags\": torch.tensor(padded_tags),\n",
    "        }\n",
    "        \n",
    "    def __find_max_seq_len(self) -> None:\n",
    "        seq_lens = []\n",
    "        \n",
    "        for idx in range(len(self.data)):\n",
    "            words = [i.word for i in self.data[idx].info]\n",
    "            seq_lens.append(len(words))\n",
    "        \n",
    "        \n",
    "        self.MAX_SEQ_LEN = max(seq_lens)\n",
    "        \n",
    "    def __encode_tags(self) -> None:\n",
    "        for idx, tag in enumerate(self.UNIQUE_TAGS):\n",
    "            self.tag_dict[tag] = idx\n",
    "        \n",
    "    def __read_data(self) -> None:\n",
    "        with open(self.filename, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            \n",
    "            # ============ read the text file =============\n",
    "            lines = list()\n",
    "            buffer = list()\n",
    "            for _, line in tqdm(enumerate(data)):\n",
    "                if line == \"\\n\":\n",
    "                    lines.append(buffer)\n",
    "                    buffer = []\n",
    "                else:\n",
    "                    buffer.append(line)\n",
    "                    \n",
    "            # ============== organize in objects ==============\n",
    "            for idx, line in tqdm(enumerate(lines)):\n",
    "                # from line index 2 and onwards\n",
    "                line_info = list()\n",
    "                for info in line[2:]:\n",
    "                    buffer = info.split(\"\\t\")\n",
    "                \n",
    "                    try:\n",
    "                        word = buffer[1]\n",
    "                        lemma = buffer[2]\n",
    "                        tag = buffer[3]\n",
    "                        \n",
    "                        line_info.append(ConlluRowInfo(word, lemma, tag))\n",
    "                        \n",
    "                    except IndexError:\n",
    "                        print(buffer)\n",
    "                        \n",
    "                \n",
    "                lines[idx] = ConlluRow(line_info)    \n",
    "\n",
    "            self.data = lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72ef661397d40d7808ee1eeae203ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1c948aafe64d22b0cebc6008668284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'words': tensor([3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000,\n",
       "         3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000, 3000000,\n",
       "         3000000, 3000000, 3000000,   31905, 3000001, 3000001,   12654,   14263,\n",
       "         3000001,      20,     190,      42,   40105,       1,     234,   22860,\n",
       "         3000001,      86,     951,     177,      48,      45,       4,    2604,\n",
       "            2747, 3000001,      20,     248, 3000001, 3000001, 3000001,   10297,\n",
       "         3000001, 3000001], dtype=torch.int32),\n",
       " 'tags': tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "         17, 13, 13, 12, 14,  2, 12,  0,  7,  7, 15,  6,  1,  2,  3,  4,  4, 15,\n",
       "         10,  1,  7,  4,  0, 12,  0,  7, 10,  2, 12,  8, 13, 12],\n",
       "        dtype=torch.int32)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TweebankDataset(train_file)\n",
    "sample = dataset[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRON': 0,\n",
       " 'NUM': 1,\n",
       " 'NOUN': 2,\n",
       " 'CCONJ': 3,\n",
       " 'ADV': 4,\n",
       " 'SCONJ': 5,\n",
       " 'ADP': 6,\n",
       " 'AUX': 7,\n",
       " 'PROPN': 8,\n",
       " 'SYM': 9,\n",
       " 'DET': 10,\n",
       " 'INTJ': 11,\n",
       " 'PUNCT': 12,\n",
       " 'X': 13,\n",
       " 'ADJ': 14,\n",
       " 'VERB': 15,\n",
       " 'PART': 16,\n",
       " '</PAD>': 17}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.582Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackabuse.com/python-how-to-flatten-list-of-lists/\n",
    "\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# all_tags = [data[\"tags\"] for data in dataset]\n",
    "# all_tags = list(itertools.chain(*all_tags))\n",
    "# unique_tags = set(all_tags)\n",
    "# print(list(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9045530e2cb4c3087988a9e9992c13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d5d5b6ebb94d629df1beddb2504194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bs = 128\n",
    "dl_args = {\n",
    "    \"pin_memory\": True,\n",
    "    \"batch_size\": bs\n",
    "}\n",
    "\n",
    "\n",
    "training_set = dataset\n",
    "validation_set = TweebankDataset(\"/mnt/Others/experiments/datasets/Tweebank-dev/converted/en-ud-tweet-dev.fixed.conllu\")\n",
    "# test_set = TweebankDataset(\"/mnt/Others/experiments/datasets/Tweebank-dev/converted/en-ud-tweet-test.fixed.conllu\")\n",
    "\n",
    "train_loader = DataLoader(training_set, shuffle=True, **dl_args)\n",
    "val_loader = DataLoader(validation_set, shuffle=False, **dl_args)\n",
    "# test_loader = DataLoader(test_set, shuffle=False, **dl_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.583Z"
    }
   },
   "outputs": [],
   "source": [
    "# assert training_set.tag_dict == validation_set.tag_dict == test_set.tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.584Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim: int, \n",
    "                 hidden_dim: int,  \n",
    "                 tagset_size: int,\n",
    "                 padding_idx=padding_idx, \n",
    "                 freeze_embeddings=True, \n",
    "                 w2v_weights=gnews_embeddings) -> None:\n",
    "        \n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.taget_size = tagset_size\n",
    "        \n",
    "        \n",
    "        embedding_tensors = torch.from_numpy(w2v_weights.vectors) # type: ignore        \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(\n",
    "            embedding_tensors, freeze=freeze_embeddings, padding_idx=padding_idx)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "        \n",
    "        self.attention =  nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=0.1, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "        \n",
    "    def forward(self, words):\n",
    "        embeds = self.word_embeddings(words)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        \n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        relu_out = self.relu(attn_out)\n",
    "        \n",
    "        linear_out = self.linear(relu_out)\n",
    "\n",
    "        logits = F.log_softmax(linear_out, dim=-1)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.584Z"
    }
   },
   "outputs": [],
   "source": [
    "tagset_size = len(dataset.UNIQUE_TAGS)\n",
    "model = LSTMTagger(embedding_dim=300, hidden_dim=100,  tagset_size=tagset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out :: torch.Size([1, 18, 50])\n",
      "tags :: torch.Size([1, 50])\n",
      "tensor(2.8996)\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "# run a sample forward pass\n",
    "sample = dataset[42]\n",
    "\n",
    "with torch.no_grad():\n",
    "    words = sample[\"words\"].unsqueeze(0)\n",
    "    tags = sample[\"tags\"].unsqueeze(0).long()\n",
    "    \n",
    "    out = model(words)\n",
    "    \n",
    "    # apparently nllloss expects inputs in shape (bs, n_classes, feature_dims.......)\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss\n",
    "    out = rearrange(out, \"1 words probas -> 1 probas words\")\n",
    "    \n",
    "    print(f\"out :: {out.size()}\")\n",
    "    print(f\"tags :: {tags.size()}\")\n",
    "    \n",
    "# sample_loss = F.nll_loss(out, tags, ignore_index=17)\n",
    "sample_loss = nn.NLLLoss(ignore_index=17)\n",
    "print(sample_loss(input=out, target=tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 2, 1])\n",
      "tensor([[-2.0379, -1.0099,  0.7737],\n",
      "        [ 0.4582, -0.0672,  0.0468],\n",
      "        [-0.1913, -0.7795,  0.3166],\n",
      "        [-0.4165, -1.1253,  0.9711]])\n",
      "tensor(1.0230)\n"
     ]
    }
   ],
   "source": [
    "# https://discuss.pytorch.org/t/loss-function-for-multi-class-with-probabilities-as-output/60866\n",
    "\n",
    "x = torch.tensor([[0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 0]], dtype=torch.int32)\n",
    "_, i = torch.max(x, dim=-1)\n",
    "\n",
    "print(i)\n",
    "\n",
    "y = torch.randn((4, 3), dtype=torch.float32)\n",
    "print(y)\n",
    "\n",
    "print(F.nll_loss(F.log_softmax(y, dim=-1), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.586Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7c4fc6784e4ba9807572f655c761e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:: [1]/[20] Step:: 0\n",
      "Train Loss:: 2.9020349979400635 __________ Val Loss:: 2.9007673263549805\n",
      "Epoch:: [1]/[20] Step:: 10\n",
      "Train Loss:: 2.605685234069824 __________ Val Loss:: 2.61468243598938\n",
      "Epoch:: [2]/[20] Step:: 0\n",
      "Train Loss:: 2.5886306762695312 __________ Val Loss:: 2.583510637283325\n",
      "Epoch:: [2]/[20] Step:: 10\n",
      "Train Loss:: 2.433333396911621 __________ Val Loss:: 2.3958187103271484\n",
      "Epoch:: [3]/[20] Step:: 0\n",
      "Train Loss:: 2.3342514038085938 __________ Val Loss:: 2.3139851093292236\n",
      "Epoch:: [3]/[20] Step:: 10\n",
      "Train Loss:: 2.29109787940979 __________ Val Loss:: 2.2070090770721436\n",
      "Epoch:: [4]/[20] Step:: 0\n",
      "Train Loss:: 2.2596089839935303 __________ Val Loss:: 2.1938652992248535\n",
      "Epoch:: [4]/[20] Step:: 10\n",
      "Train Loss:: 2.160061836242676 __________ Val Loss:: 2.1366758346557617\n",
      "Epoch:: [5]/[20] Step:: 0\n",
      "Train Loss:: 2.1164329051971436 __________ Val Loss:: 2.1010937690734863\n",
      "Epoch:: [5]/[20] Step:: 10\n",
      "Train Loss:: 2.039097547531128 __________ Val Loss:: 1.9936223030090332\n",
      "Epoch:: [6]/[20] Step:: 0\n",
      "Train Loss:: 2.0051937103271484 __________ Val Loss:: 1.9610109329223633\n",
      "Epoch:: [6]/[20] Step:: 10\n",
      "Train Loss:: 1.881091833114624 __________ Val Loss:: 1.8477367162704468\n",
      "Epoch:: [7]/[20] Step:: 0\n",
      "Train Loss:: 1.829955816268921 __________ Val Loss:: 1.8112372159957886\n",
      "Epoch:: [7]/[20] Step:: 10\n",
      "Train Loss:: 1.7343958616256714 __________ Val Loss:: 1.7122373580932617\n",
      "Epoch:: [8]/[20] Step:: 0\n",
      "Train Loss:: 1.669018268585205 __________ Val Loss:: 1.6867351531982422\n",
      "Epoch:: [8]/[20] Step:: 10\n",
      "Train Loss:: 1.6205811500549316 __________ Val Loss:: 1.5740537643432617\n",
      "Epoch:: [9]/[20] Step:: 0\n",
      "Train Loss:: 1.5319714546203613 __________ Val Loss:: 1.5402030944824219\n",
      "Epoch:: [9]/[20] Step:: 10\n",
      "Train Loss:: 1.3942142724990845 __________ Val Loss:: 1.4211570024490356\n",
      "Epoch:: [10]/[20] Step:: 0\n",
      "Train Loss:: 1.4091618061065674 __________ Val Loss:: 1.384534478187561\n",
      "Epoch:: [10]/[20] Step:: 10\n",
      "Train Loss:: 1.2663216590881348 __________ Val Loss:: 1.2404429912567139\n",
      "Epoch:: [11]/[20] Step:: 0\n",
      "Train Loss:: 1.1813452243804932 __________ Val Loss:: 1.2207757234573364\n",
      "Epoch:: [11]/[20] Step:: 10\n",
      "Train Loss:: 1.1116256713867188 __________ Val Loss:: 1.1492180824279785\n",
      "Epoch:: [12]/[20] Step:: 0\n",
      "Train Loss:: 1.1113348007202148 __________ Val Loss:: 1.1513723134994507\n",
      "Epoch:: [12]/[20] Step:: 10\n",
      "Train Loss:: 1.116006851196289 __________ Val Loss:: 1.1016831398010254\n",
      "Epoch:: [13]/[20] Step:: 0\n",
      "Train Loss:: 1.0706156492233276 __________ Val Loss:: 1.1008292436599731\n",
      "Epoch:: [13]/[20] Step:: 10\n",
      "Train Loss:: 0.9459255933761597 __________ Val Loss:: 1.0546274185180664\n",
      "Epoch:: [14]/[20] Step:: 0\n",
      "Train Loss:: 1.0560803413391113 __________ Val Loss:: 1.0513118505477905\n",
      "Epoch:: [14]/[20] Step:: 10\n",
      "Train Loss:: 0.979905903339386 __________ Val Loss:: 1.0036107301712036\n",
      "Epoch:: [15]/[20] Step:: 0\n",
      "Train Loss:: 0.9836480021476746 __________ Val Loss:: 0.9975724816322327\n",
      "Epoch:: [15]/[20] Step:: 10\n",
      "Train Loss:: 0.9793418049812317 __________ Val Loss:: 0.9948780536651611\n",
      "Epoch:: [16]/[20] Step:: 0\n",
      "Train Loss:: 0.9356094002723694 __________ Val Loss:: 0.9934573173522949\n",
      "Epoch:: [16]/[20] Step:: 10\n",
      "Train Loss:: 0.9132127165794373 __________ Val Loss:: 0.9670546650886536\n",
      "Epoch:: [17]/[20] Step:: 0\n",
      "Train Loss:: 0.9749714732170105 __________ Val Loss:: 0.9594925045967102\n",
      "Epoch:: [17]/[20] Step:: 10\n",
      "Train Loss:: 0.8906992077827454 __________ Val Loss:: 0.9540648460388184\n",
      "Epoch:: [18]/[20] Step:: 0\n",
      "Train Loss:: 0.8571797013282776 __________ Val Loss:: 0.9324821829795837\n",
      "Epoch:: [18]/[20] Step:: 10\n",
      "Train Loss:: 0.8434054851531982 __________ Val Loss:: 0.9343039393424988\n",
      "Epoch:: [19]/[20] Step:: 0\n",
      "Train Loss:: 0.8694400191307068 __________ Val Loss:: 0.9367151260375977\n",
      "Epoch:: [19]/[20] Step:: 10\n",
      "Train Loss:: 0.8348217606544495 __________ Val Loss:: 0.9166299700737\n",
      "Epoch:: [20]/[20] Step:: 0\n",
      "Train Loss:: 0.8502605557441711 __________ Val Loss:: 0.9283155798912048\n",
      "Epoch:: [20]/[20] Step:: 10\n",
      "Train Loss:: 0.7347011566162109 __________ Val Loss:: 0.9175626635551453\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(params=model.parameters())\n",
    "# ignore the index for PAD\n",
    "criterion = nn.NLLLoss(ignore_index=training_set.tag_dict.get(\"</PAD>\"))  # type: ignore        \n",
    "run_validation_every_n_step = 10\n",
    "\n",
    "\n",
    "# fp16\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "epochs = 20\n",
    "for e in trange(epochs):\n",
    "\n",
    "    steps = 0\n",
    "    for batch in train_loader:\n",
    "        # switch to train mode\n",
    "        model.train()\n",
    "        \n",
    "        words = batch[\"words\"]\n",
    "        tags = batch[\"tags\"].long()\n",
    "        \n",
    "        # send data to device\n",
    "        words = words.to(device)\n",
    "        tags = tags.to(device)\n",
    "        \n",
    "        # zero out optimizer to accumulate new grads\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            logits = model(words)\n",
    "            logits = rearrange(logits, \"bs words probas -> bs probas words\")\n",
    "            \n",
    "            # loss\n",
    "            loss = criterion(logits, tags)\n",
    "        \n",
    "        \n",
    "        # ======== validation ==============\n",
    "        if steps % run_validation_every_n_step == 0:\n",
    "            val_losses = []\n",
    "            \n",
    "            # switch context\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    words = val_batch[\"words\"]\n",
    "                    tags = val_batch[\"tags\"].long()\n",
    "                    \n",
    "                    words = words.to(device)\n",
    "                    tags = tags.to(device)\n",
    "                    \n",
    "                    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                        logits = model(words)\n",
    "                        logits = rearrange(logits, \"bs words probas -> bs probas words\")\n",
    "                        val_loss = criterion(logits, tags)\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                    # preds = torch.max(logits, dim=-1).indices\n",
    "\n",
    "                # log\n",
    "                print(f\"Epoch:: [{e + 1}]/[{epochs}] Step:: {steps}\")\n",
    "                print(f\"Train Loss:: {loss} __________ Val Loss:: {torch.mean(torch.tensor(val_losses))}\")\n",
    "        \n",
    "        # switch context\n",
    "        model.train()\n",
    "        scaler.scale(loss).backward()  # type: ignore\n",
    "        # loss.backward()\n",
    "        scaler.step(optimizer)\n",
    "        # optimizer.step()\n",
    "        scaler.update()\n",
    "        steps += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-25T21:39:55.587Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"saved.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5383723a85d76e4e9cac0d136e01f6d8a177dc9dc4ee2b9a35edd51227ec1b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
