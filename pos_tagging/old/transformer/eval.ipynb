{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "# dataset\n",
    "data_root = os.path.join(\n",
    "    os.getenv(\"EXPSTORE\"),\n",
    "    \"datasets\",\n",
    "    \"Tweebank-dev\",\n",
    "    \"converted\"\n",
    ")\n",
    "\n",
    "assert os.path.exists(data_root) == True\n",
    "\n",
    "data_splits = {\n",
    "    \"train\": \"en-ud-tweet-train.fixed.conllu\",\n",
    "    \"val\": \"en-ud-tweet-dev.fixed.conllu\",\n",
    "    \"test\": \"en-ud-tweet-test.fixed.conllu\"\n",
    "}\n",
    "\n",
    "\n",
    "class ConlluRowInfo:\n",
    "    word: str\n",
    "    lemma: str\n",
    "    pos: str\n",
    "\n",
    "    def __init__(self, word: str, lemma: str, pos: str) -> None:\n",
    "        self.word = word\n",
    "        self.lemma = lemma\n",
    "        self.pos = pos\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        rep = {\n",
    "            \"word\": self.word,\n",
    "            \"lemma\": self.lemma,\n",
    "            \"pos\": self.pos\n",
    "        }\n",
    "        return str(rep)\n",
    "\n",
    "\n",
    "class ConlluRow:\n",
    "    info: List[ConlluRowInfo]\n",
    "    # text: str\n",
    "\n",
    "    def __init__(self, infos: List[ConlluRowInfo]) -> None:\n",
    "        self.info = infos\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"info : {self.info}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "def read_data(filename):\n",
    "    # ============ read ==============\n",
    "    with open(filename, \"r\") as f:\n",
    "        raw_data = f.readlines()\n",
    "\n",
    "    # =============== process =============\n",
    "    lines = list()\n",
    "    buffer = list()\n",
    "    for _, line in tqdm(enumerate(raw_data), desc=\"reading lines from file\"):\n",
    "        if line == \"\\n\":\n",
    "            lines.append(buffer)\n",
    "            buffer = list()\n",
    "        else:\n",
    "            buffer.append(line)\n",
    "\n",
    "    # make sure that buffer is always empty after the loop ends\n",
    "    assert len(buffer) == 0\n",
    "\n",
    "    # ========== orga in objects ============\n",
    "    processed_lines = list()\n",
    "    for idx, l in tqdm(enumerate(lines), desc=\"organising in objects\"):\n",
    "        l_info = list()\n",
    "        for info in l[2:]:\n",
    "            temp = info.split(\"\\t\")\n",
    "\n",
    "            # need idx 1, 2,3 : word, lemma and pos\n",
    "            word = temp[1]\n",
    "            lemma = temp[2]\n",
    "            tag = temp[3]\n",
    "\n",
    "            l_info.append(ConlluRowInfo(word, lemma, tag))\n",
    "\n",
    "        processed_lines.append(ConlluRow(l_info))\n",
    "\n",
    "    # ===========================================\n",
    "    return processed_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 128) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        positions = torch.arange(0, max_len)\n",
    "        # vectorize\n",
    "        positions = rearrange(positions, \"n -> n 1\")\n",
    "\n",
    "        # but in log space\n",
    "        # -2i * n / d\n",
    "        # even steps , since 2i\n",
    "        # n = 10e3\n",
    "        denominator = -torch.arange(0, d_model, 2) * \\\n",
    "            torch.log(torch.tensor(10.0e3) / d_model)\n",
    "        # exp since we took log from the original equation, which was 1/n^(2i / d)\n",
    "        denominator = torch.exp(denominator)\n",
    "\n",
    "        # positional encoding tensor\n",
    "        pe = torch.zeros(size=(max_len, 1, d_model))\n",
    "\n",
    "        # encode the first dim\n",
    "        pe[:, 0, 0::2] = torch.sin(positions * denominator)\n",
    "        # second dim\n",
    "        pe[:, 0, 1::2] = torch.cos(positions * denominator)\n",
    "\n",
    "        # register as a buffer, variable but without gradient update\n",
    "        self.register_buffer(\"positional_encoding\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x has a shape of (seq_len, batch_size, embedding_dim)\n",
    "        # so you pass the embedded vectors for a sequence\n",
    "\n",
    "        # residual connection + dropout\n",
    "        x = x + self.positional_encoding[:x.size(0)]  # type: ignore\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class PosTaggerTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 d_model: int,\n",
    "                 n_heads: int,\n",
    "                 n_encoder_layers: int,\n",
    "                 n_decoder_layers: int,\n",
    "                 dropout: float,\n",
    "                 max_len: int,\n",
    "                 n_tags: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            d_model=self.d_model, max_len=max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=n_heads,\n",
    "            num_decoder_layers=n_encoder_layers,\n",
    "            num_encoder_layers=n_decoder_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(d_model, n_tags)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        # ================ get embed ================\n",
    "        src = self.embedding(source) * torch.sqrt(torch.tensor(self.d_model))\n",
    "        tgt = self.embedding(target) * torch.sqrt(torch.tensor(self.d_model))\n",
    "\n",
    "        # ================ rearrange the shapes =================\n",
    "        # default shape : bs, seq, embed\n",
    "        # transformer needs : seq, bs, embed\n",
    "        # can also use permute but this approach is more intuitive\n",
    "        src = rearrange(src, \"bs seq embed -> seq bs embed\")\n",
    "        tgt = rearrange(tgt, \"bs seq embed -> seq bs embed\")\n",
    "\n",
    "        # =================== pos enc ================\n",
    "        src_pe = self.positional_encoder(src)\n",
    "        tgt_pe = self.positional_encoder(tgt)\n",
    "\n",
    "        # =========== pass through transformer =============\n",
    "        out = self.transformer(src_pe, tgt_pe)\n",
    "\n",
    "        # ================ final linear layer ================\n",
    "        out = self.linear(out)\n",
    "        out = self.log_softmax(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TweebankDataset(Dataset):\n",
    "    def __init__(self, file_name: str, max_seq_len: int, file_reader_fn=read_data) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.MAX_SEQ_LEN = max_seq_len\n",
    "\n",
    "        # ================ tags ===================\n",
    "\n",
    "        self.UNIQUE_TAGS = ['PRON', 'NUM', 'NOUN', 'CCONJ', 'ADV', 'SCONJ',\n",
    "                            'ADP', 'AUX', 'PROPN', 'SYM', 'DET',\n",
    "                            'INTJ', 'PUNCT', 'X', 'ADJ', 'VERB', 'PART', '</PAD>']\n",
    "        self.tag_dict = dict()\n",
    "        self.__encode_tags()\n",
    "\n",
    "        self.n_classes = len(self.UNIQUE_TAGS)\n",
    "\n",
    "        # ================= data ===================\n",
    "        self.data = file_reader_fn(file_name)\n",
    "\n",
    "        # ============== vocab =====================\n",
    "        self.vocab = list()\n",
    "        self.__build_vocab()\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.word_dict = dict()\n",
    "        self.__encode_words()\n",
    "\n",
    "    # ======================= tag encoding ===============\n",
    "    def __encode_tags(self) -> None:\n",
    "        for idx, tag in enumerate(self.UNIQUE_TAGS):\n",
    "            self.tag_dict[tag] = idx\n",
    "\n",
    "    # ======================= vocab building and encoding ===============\n",
    "    def __build_vocab(self) -> None:\n",
    "        vocabulary = set()\n",
    "        for idx in range(len(self.data)):\n",
    "            words = [i.word for i in self.data[idx].info]\n",
    "            for w in words:\n",
    "                vocabulary.add(w)\n",
    "\n",
    "        # ============ add oov and pad ===============\n",
    "        vocabulary.add(\"</OOV>\")\n",
    "        vocabulary.add(\"</PAD>\")\n",
    "        self.vocab = list(vocabulary)\n",
    "\n",
    "    def __encode_words(self) -> None:\n",
    "        for idx, word in enumerate(self.vocab):\n",
    "            self.word_dict[word] = idx\n",
    "\n",
    "    # ========================== dataset methods =================\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = [i.word for i in self.data[idx].info]\n",
    "        tags = [i.pos for i in self.data[idx].info]\n",
    "\n",
    "        # ============ convert to ids =================\n",
    "        # using idx from vocab\n",
    "        oov_idx = self.word_dict[\"</OOV>\"]\n",
    "        word_ids = list()\n",
    "\n",
    "        for w in words:\n",
    "            if w in self.word_dict.keys():\n",
    "                word_ids.append(self.word_dict[w])\n",
    "            else:\n",
    "                word_ids.append(oov_idx)\n",
    "\n",
    "        # ================ same for tags ============\n",
    "        tag_ids = list()\n",
    "        for t in tags:\n",
    "            tag_ids.append(self.tag_dict[t])\n",
    "\n",
    "        # ============== pad words ===============\n",
    "        padding_idx = self.word_dict[\"</PAD>\"]\n",
    "\n",
    "        # left pad\n",
    "        padded_words = torch.ones(\n",
    "            self.MAX_SEQ_LEN, dtype=torch.long) * padding_idx\n",
    "        padded_words[-len(word_ids):] = torch.tensor(word_ids)\n",
    "\n",
    "        padded_tags = torch.ones(\n",
    "            self.MAX_SEQ_LEN, dtype=torch.long) * self.tag_dict.get(\"</PAD>\")  # type: ignore\n",
    "        padded_tags[-len(tags):] = torch.tensor(tag_ids)\n",
    "\n",
    "        return {\n",
    "            \"source\": padded_words,\n",
    "            \"targets\": padded_tags\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75625bba11df48dca1f9205462aaa951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading lines from file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b124c635b14c51ab89cc40ebd09ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "organising in objects: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20eb8952a1942edb32415352772ec27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading lines from file: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e163a39cc04725ac7baad621866c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "organising in objects: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# device\n",
    "device = \"cuda\"\n",
    "\n",
    "# hparams\n",
    "max_len = 128\n",
    "d_model = 512\n",
    "\n",
    "\n",
    "trainset = TweebankDataset(os.path.join(\n",
    "    data_root, data_splits[\"train\"]), max_len)\n",
    "testset = TweebankDataset(os.path.join(\n",
    "    data_root, data_splits[\"test\"]), max_len)\n",
    "\n",
    "train_vocab_size = trainset.vocab_size\n",
    "\n",
    "\n",
    "data_loader_args = {\n",
    "    \"pin_memory\": True,\n",
    "    \"batch_size\": 64,\n",
    "}\n",
    "\n",
    "test_loader = DataLoader(testset, shuffle=False, **data_loader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosTaggerTransformer(\n",
       "  (embedding): Embedding(8566, 512)\n",
       "  (positional_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=18, bias=True)\n",
       "  (log_softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PosTaggerTransformer(\n",
    "    vocab_size=train_vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=4,\n",
    "    n_encoder_layers=4,\n",
    "    n_decoder_layers=4,\n",
    "    dropout=0.1,\n",
    "    max_len=max_len,\n",
    "    n_tags=trainset.n_classes)\n",
    "\n",
    "model.load_state_dict(torch.load(\"saved.pt\"))\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on a single batch\n",
    "def categorical_accuracy(pred: torch.Tensor, true: torch.Tensor, pad_idx: int) -> torch.Tensor:\n",
    "    # unvectorized, runs on a single data instance\n",
    "    def ca(pred: torch.Tensor, true: torch.Tensor) -> torch.Tensor:\n",
    "        non_pad_idx = (true != pad_idx).nonzero()\n",
    "        acc = pred[non_pad_idx].squeeze(0).eq(true[non_pad_idx])\n",
    "\n",
    "        return acc.sum() / true[non_pad_idx].size()[0]\n",
    "\n",
    "    bs = true.size()[0]\n",
    "    acc = torch.zeros(size=(bs,), dtype=torch.float32)\n",
    "    for i in range(bs):\n",
    "        p = pred[i]\n",
    "        t = true[i]\n",
    "\n",
    "        acc[i] = ca(p, t)\n",
    "\n",
    "    return acc.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy : 1.0 for # 19 batches\n"
     ]
    }
   ],
   "source": [
    "# on a single batch\n",
    "def categorical_accuracy(pred: torch.Tensor, true: torch.Tensor, pad_idx: int) -> torch.Tensor:\n",
    "    # unvectorized, runs on a single data instance\n",
    "    def ca(pred: torch.Tensor, true: torch.Tensor) -> torch.Tensor:\n",
    "        non_pad_idx = (true != pad_idx).nonzero()\n",
    "        acc = pred[non_pad_idx].squeeze(0).eq(true[non_pad_idx])\n",
    "\n",
    "        return acc.sum() / true[non_pad_idx].size()[0]\n",
    "\n",
    "    bs = true.size()[0]\n",
    "    acc = torch.zeros(size=(bs,), dtype=torch.float32)\n",
    "    for i in range(bs):\n",
    "        p = pred[i]\n",
    "        t = true[i]\n",
    "\n",
    "        acc[i] = ca(p, t)\n",
    "\n",
    "    return acc.mean()\n",
    "\n",
    "\n",
    "def evaluate(model: PosTaggerTransformer, data_loader: DataLoader, pad_idx: int) -> None:\n",
    "    all_accs = list()\n",
    "\n",
    "    PAD_IDX = pad_idx\n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        source = batch[\"source\"].to(device)\n",
    "        targets = batch[\"targets\"].long().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(source, targets)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            preds = rearrange(preds, \"seq bs -> bs seq\")\n",
    "\n",
    "        # categorical acc\n",
    "        a = categorical_accuracy(preds, targets, PAD_IDX)\n",
    "        all_accs.append(a)\n",
    "\n",
    "    all_accs = torch.tensor(all_accs)\n",
    "    print(\n",
    "        f\"Mean Accuracy : {all_accs.mean()} for # {all_accs.size()[0]} batches\")\n",
    "\n",
    "\n",
    "evaluate(model, test_loader, testset.tag_dict.get(\"</PAD>\"))  # type: ignore\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5383723a85d76e4e9cac0d136e01f6d8a177dc9dc4ee2b9a35edd51227ec1b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
