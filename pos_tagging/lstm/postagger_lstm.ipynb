{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "734bcaf9-a721-47ba-bf12-d1f46d55389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"batterydata/pos_tagging\"\n",
    "training_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "test_dataset = load_dataset(dataset_name, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "207d6743-6ab8-44ea-84c9-b5e97c8171f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dict containing word -> idx mapping\n",
    "def create_word_indices(dataset):\n",
    "    unique_words = set()\n",
    "    word_to_idx = dict()\n",
    "    # add an out of vocab token\n",
    "    oov_token = \"<OOV>\"\n",
    "    pad_token = \"<PAD>\"\n",
    "    word_to_idx[oov_token] = 0\n",
    "    word_to_idx[pad_token] = 1\n",
    "    \n",
    "    # find unique words\n",
    "    for data in dataset:\n",
    "        words = data[\"words\"]\n",
    "        for w in words:\n",
    "            unique_words.add(w)\n",
    "            \n",
    "    # add index to them\n",
    "    for idx, uw in enumerate(list(unique_words)):\n",
    "        word_to_idx[uw] = idx + 2 # since oov is at 0 and pad at 1\n",
    "        \n",
    "    \n",
    "    return word_to_idx\n",
    "\n",
    "\n",
    "# ===============\n",
    "word_to_idx = create_word_indices(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8d8aaa-a1ea-4bfb-88a4-41de723f6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_to_idx(dataset):\n",
    "    unique_labels = set()\n",
    "    label_to_idx = dict()\n",
    "    # add an out of vocab token\n",
    "    oov_token = \"<OOV>\"\n",
    "    pad_token = \"<PAD>\"\n",
    "    label_to_idx[oov_token] = 0\n",
    "    label_to_idx[pad_token] = 1\n",
    "    \n",
    "    # find the labels\n",
    "    for data in dataset:\n",
    "        labels = data[\"labels\"]\n",
    "        for l in labels:\n",
    "            unique_labels.add(l)\n",
    "            \n",
    "    # index\n",
    "    for idx, label in enumerate(list(unique_labels)):\n",
    "        label_to_idx[label] = idx + 2\n",
    "        \n",
    "    return label_to_idx\n",
    "    \n",
    "label_to_idx = create_label_to_idx(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9eca9b9-9fe5-4bab-baaf-922f7ca73edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for a single instance\n",
    "def encode_data_instance(data, word_to_idx, label_to_idx):\n",
    "    words = [\n",
    "        word_to_idx.get(word, word_to_idx[\"<OOV>\"]) for word in data[\"words\"]\n",
    "    ]\n",
    "    \n",
    "    labels = [\n",
    "        label_to_idx[label] for label in data[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"words\": words,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9840de40-1545-4229-9002-45a59012f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = map(lambda data: encode_data_instance(data, word_to_idx, label_to_idx), training_dataset)\n",
    "trainset = list(trainset)\n",
    "\n",
    "\n",
    "\n",
    "testset = map(lambda data: encode_data_instance(\n",
    "    data, word_to_idx, label_to_idx), test_dataset)\n",
    "testset = list(testset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f1b8661-968d-454a-89e2-8bd37260c604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9138\n",
      "3916\n"
     ]
    }
   ],
   "source": [
    "# now to create the validation set\n",
    "import numpy as np\n",
    "\n",
    "def create_train_validation_splits(trainset, validation_ratio):\n",
    "    validation_set_size = int(len(trainset) * validation_ratio)\n",
    "    validation_indices = np.random.choice(len(trainset), replace=False, size=validation_set_size).tolist()\n",
    "    \n",
    "    # now to separate trainset indices\n",
    "    trainset_indices = [i for i in range(len(trainset)) if i not in validation_indices]\n",
    "    \n",
    "    return trainset_indices, validation_indices\n",
    "\n",
    "\n",
    "trainset_indices, validation_indices = create_train_validation_splits(trainset, 0.3)\n",
    "\n",
    "print(len(trainset_indices))\n",
    "print(len(validation_indices))\n",
    "\n",
    "\n",
    "assert len(trainset_indices) + len(validation_indices) == len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70540d79-efed-41ed-bf1c-bdd0705b5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "\n",
    "class TagDataset(Dataset):\n",
    "    def __init__(self, indices, dataset) -> None:\n",
    "        self.indices = indices\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.indices is None:\n",
    "            # this is for the test case\n",
    "            return len(self.dataset)\n",
    "        else:\n",
    "            return len(self.indices)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.indices is None:\n",
    "            idx = index\n",
    "        else:\n",
    "            idx = self.indices[index]\n",
    "            \n",
    "        data = self.dataset[idx]\n",
    "        \n",
    "        words = data[\"words\"]\n",
    "        labels = data[\"labels\"]\n",
    "        \n",
    "        # padding to 300\n",
    "        # pad token idx is 1\n",
    "        # words = np.ones((300, ), dtype=np.int32)\n",
    "        # words[:len(data[\"words\"])] = data[\"words\"] \n",
    "    \n",
    "        \n",
    "        # labels = np.ones((300, ), dtype=np.int32)\n",
    "        # labels[:len(data[\"labels\"])] = data[\"labels\"]\n",
    "        \n",
    "        \n",
    "        return torch.tensor(words).long(), torch.tensor(labels).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9277be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "  (xx, yy) = zip(*batch)\n",
    "  x_lens = [len(x) for x in xx]\n",
    "  y_lens = [len(y) for y in yy]\n",
    "\n",
    "  xx_pad = pad_sequence(xx, batch_first=True, padding_value=1)\n",
    "  yy_pad = pad_sequence(yy, batch_first=True, padding_value=1)\n",
    "\n",
    "  return xx_pad, yy_pad, x_lens, y_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "148b2a95-6290-48cf-910b-c7c63629be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TagDataset(trainset_indices, trainset), batch_size=128, shuffle=True, collate_fn=pad_collate)\n",
    "val_loader = DataLoader(\n",
    "    TagDataset(validation_indices, trainset), batch_size=128, shuffle=False, collate_fn=pad_collate)\n",
    "test_loader = DataLoader(\n",
    "    TagDataset(None, testset), batch_size=128, shuffle=False, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f414fb5d-a6ed-4c1c-97d0-563e60cacddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 56])\n",
      "torch.Size([128, 56])\n"
     ]
    }
   ],
   "source": [
    "# =========== test a dataloader ==========\n",
    "for batch in train_loader:\n",
    "    w, l, ws, ls = batch\n",
    "    print(w.size())\n",
    "    print(l.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dc555f5-09df-4eed-8d9d-1e007cc8135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ef9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32953c4f-1c81-4faa-aeab-f67ac10574bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 7.5 M \n",
      "1 | lstm      | LSTM      | 722 K \n",
      "2 | fc        | Linear    | 15.1 K\n",
      "3 | dropout   | Dropout   | 0     \n",
      "----------------------------------------\n",
      "8.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.2 M     Total params\n",
      "32.769    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcb292066db47529ee428cf5a81f13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawon/miniconda3/envs/exp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "/home/shawon/miniconda3/envs/exp/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93eb3d853ff47ffb7eaa4694b87772e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1203335a57fa40309c3fbd7591a84b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df3464dbea0469d8f402549ba55ce73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e731157d5744edf88c44f5f5e5e778f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9694ea1cb6f84b94856a58f6f2499ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c00d29bdb34e2a963c0fb60f87c17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de22db83d5e4f73ab0e3cbe2ca54ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5838543611472480988b5187b0a80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae632a1f83b34fec8069930e21693d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9ffba1e1124082a889951914b84ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c167e527f14a96bc3243d0884917c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from typing import Any\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "from tqdm.auto import trange, tqdm\n",
    "from einops import rearrange\n",
    "import lightning.pytorch as L\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class LSTMTagger(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dimension, projection_dims, n_labels, pad_idx) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # hparams\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.projection_dims = projection_dims\n",
    "        self.n_labels = n_labels\n",
    "        self.pad_idx = pad_idx\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # modules\n",
    "        self.embedding = nn.Embedding(self.vocab_size, \n",
    "                                      self.embedding_dimension, \n",
    "                                      padding_idx=self.pad_idx)\n",
    "        self.lstm = nn.LSTM(self.embedding_dimension, self.projection_dims, batch_first=True)        \n",
    "        self.fc = nn.Linear(self.projection_dims, self.n_labels)\n",
    "        \n",
    "        # normal init\n",
    "        self.__custom_init()\n",
    "        self.embedding.weight.data[self.pad_idx] = torch.zeros(self.embedding_dimension, )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        \n",
    "    def __custom_init(self):\n",
    "        for p in self.parameters():\n",
    "            nn.init.normal_(p.data, mean=0, std=0.1)\n",
    "                \n",
    "    def forward(self, x, xlen):\n",
    "        out = self.embedding(x) \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # this reshaping changed things. :3 Well hell pytorch\n",
    "        # out = rearrange(out, \"batch L embed -> batch embed L\")\n",
    "        \n",
    "        # pack padded sequence\n",
    "        out = pack_padded_sequence(out, xlen, batch_first=True, enforce_sorted=False)        \n",
    "        out, _ = self.lstm(out)\n",
    "        \n",
    "        # back to padded \n",
    "        out, _ = pad_packed_sequence(out, batch_first=True, padding_value=1)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        out = F.leaky_relu(out)\n",
    "        \n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def compute_loss(self, batch):\n",
    "        words, labels, word_len, _ = batch\n",
    "        \n",
    "        logits = self(words, word_len)\n",
    "        # reshape logits\n",
    "        logits = rearrange(logits, \"batch seq log -> batch log seq\")\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels, ignore_index=self.pad_idx)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        return optim.AdamW(self.parameters())\n",
    "    \n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        loss = self.compute_loss(batch)\n",
    "        \n",
    "        self.log(\"Loss/Train\", loss, prog_bar=True, \n",
    "                 batch_size=batch[0].size(0))\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"log\": {\n",
    "                \"Loss/Train\": loss\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        loss = self.compute_loss(batch)\n",
    "\n",
    "        self.log(\"Loss/Validation\", loss, prog_bar=True,\n",
    "                 batch_size=batch[0].size(0))\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": loss,\n",
    "            \"log\": {\n",
    "                \"Loss/Validation\": loss\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "model = LSTMTagger(len(word_to_idx), 300, 300, len(label_to_idx), 1)\n",
    "# with torch.no_grad():\n",
    "#     for batch in train_loader:\n",
    "#         loss = model.compute_loss(batch)    \n",
    "#         print(loss)\n",
    "#         break\n",
    "\n",
    "\n",
    "# create a tensorboard logger\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"tb_logs/\")\n",
    "trainer = L.Trainer(logger=tb_logger,\n",
    "                    max_epochs=10,\n",
    "                    accelerator=\"gpu\",\n",
    "                    devices=1,\n",
    "                    precision=\"bf16-mixed\",\n",
    "                    log_every_n_steps=50)\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ac6f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy, torch eq is weird\n",
    "def categorical_accuracy(preds, actual):\n",
    "    non_pad = np.nonzero(actual != 1)\n",
    "    matches = np.equal(preds[non_pad], actual[non_pad]).sum()\n",
    "    return matches / actual[non_pad].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b9cda5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177506cff01741eab7903106253edd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9942, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5246a61114145d7bbca7aa3a3302672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9441, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4fba9b8ea548888371bae74a6ac842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9343, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def evaluate(dataloader):\n",
    "    scores = list()\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        words, labels, wlen, _ = batch\n",
    "        words = words\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(words, wlen)\n",
    "            \n",
    "        probas = logits.log_softmax(dim=-1)\n",
    "        \n",
    "        preds = probas.argmax(dim=-1)\n",
    "        \n",
    "        acc = categorical_accuracy(preds.numpy(), labels.numpy())\n",
    "        scores.append(acc)\n",
    "        \n",
    "        \n",
    "    print(torch.tensor(scores).mean(dim=-1))\n",
    "    \n",
    "\n",
    "# ================\n",
    "evaluate(train_loader)\n",
    "evaluate(val_loader)\n",
    "evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb50ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
